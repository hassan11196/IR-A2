{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and declerations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import string\n",
    "\n",
    "import re\n",
    "import math\n",
    "import pprint\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "import numpy as np\n",
    "import copy\n",
    "from copy import deepcopy\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PostingList(object):\n",
    "    def __init__(self):\n",
    "        self.total_count = 0\n",
    "        self.token = ''\n",
    "        self.occurrance = {\n",
    "#             'doc_id':0 = 'positions' : [],\n",
    "#             \n",
    "        }\n",
    "    def toJSON(self):\n",
    "        return json.dumps(self, default=lambda o: o.__dict__, \n",
    "            sort_keys=True, indent=4)\n",
    "         \n",
    "    def __repr__(self):\n",
    "        \n",
    "        return f'total_cnt : {self.total_count} docs : [{self.occurrance.keys()}]'\n",
    "    def __len__(self):\n",
    "        return len(self.occurrance)\n",
    "    def addOccurrance(self, doc_id, position):\n",
    "        self.total_count += 1\n",
    "#         print(position)\n",
    "        if doc_id not in self.occurrance.keys():\n",
    "            self.occurrance[doc_id] = []\n",
    "        self.occurrance[doc_id].append(position)\n",
    "#         self.occurrance[doc_id]['position'].append(pos)\n",
    "    \n",
    "class InvertedIndex(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.index = {}\n",
    "        self.docs = {}\n",
    "\n",
    "    def get_term_postings(self, term):\n",
    "        if term in self.index.keys():\n",
    "            return self.index[term]\n",
    "        else:\n",
    "            raise ValueError(f'{term} is not present in Index. Plese Try Again')\n",
    "            return PostingList()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.index.keys())\n",
    "    \n",
    "# vector_hash = {'term1':cnt1,,,,} // v_hash\n",
    "# vector_arr = [cnt0,cnt1,cnt2 ...] // v_arr\n",
    "\n",
    "class VectorSpaceModel(object):\n",
    "    def __init__(self):\n",
    "        self.vocab = {} # type = dict -> keys = terms in docs, values = count of terms. e.g {'global': 25, ...}\n",
    "        self.expanded_vocab = {} # type =  dict -> keys =  synonyms of terms in docs, values = count of term related to synonyms in docs e.g\n",
    "        self.group_vocab = {} # type = dict ->  groups of synonyms in docs,  values = count of all synonyms in docs. e.g {('trump','trump_card','cornet','horn'): 310}\n",
    "        \n",
    "        self.vocab_idf = {} # calculated idf of terms \n",
    "        self.group_vocab_idf = {} # calculated idf of term groups\n",
    "        \n",
    "        self.docs = {}\n",
    "        self.occurrance = {}\n",
    "        self.cdocs = {}\n",
    "        self.group_cdocs = {}\n",
    "        \n",
    "        self.index = {}\n",
    "        self.group_index = {}\n",
    "        \"\"\"\n",
    "        Type = dict -> keys = terms in docs, values = synonyms groups of term.\n",
    "        e.g \n",
    "         'close_to': ('roughly',\n",
    "                      'around',\n",
    "                      'or_so',\n",
    "                      'some',\n",
    "                      'approximately',\n",
    "                      'more_or_less',\n",
    "                      'just_about',\n",
    "                      'about',\n",
    "                      'close_to'),\n",
    "        \"\"\"\n",
    "        self.term_group = {} \n",
    "        \"\"\"\n",
    "        syn_group refers to synonyms group of a term.\n",
    "        syn1 refers to 1st synonym of synonyms group of a term. \n",
    "        Type : dict -> keys = docId, values = dict with syn_group as key and count as value\n",
    "        e.g\n",
    "            {\n",
    "                \"docId\":\n",
    "                {\n",
    "                    (syn1, syn2, syn3, ... ) : count_occurance_of_any_syn_in_group\n",
    "                }\n",
    "            }\n",
    "        \n",
    "        \"\"\"\n",
    "        self.group_docs = {}\n",
    "        \n",
    "        \n",
    "    def create_doc(self, docId):\n",
    "        self.docs[docId] = dict.fromkeys(self.vocab, 0)\n",
    "        self.group_docs[docId] = dict.fromkeys(self.group_vocab, 0)\n",
    "        \n",
    "    def get_synonyms(self, term):\n",
    "        synonyms = []\n",
    "        for syn in wordnet.synsets(term):\n",
    "            for l in syn.lemmas():\n",
    "                synonyms.append(l.name()) \n",
    "        if term not in synonyms:\n",
    "            synonyms.append(term)\n",
    "        return tuple(set(synonyms))\n",
    "    \n",
    "    def add_term(self, term, docId):\n",
    "        \n",
    "        if term in self.expanded_vocab.keys():\n",
    "            synonyms = self.term_group[term]\n",
    "            self.group_vocab[synonyms] += 1\n",
    "            for related_word in synonyms:\n",
    "                related_word= related_word.lower()\n",
    "                self.expanded_vocab[related_word] += 1\n",
    "            if synonyms not in self.group_docs[docId].keys():\n",
    "                self.group_docs[docId][synonyms] = 1\n",
    "            else:\n",
    "                self.group_docs[docId][synonyms] += 1\n",
    "            self.group_index[synonyms].add(docId)\n",
    "        \n",
    "        else:\n",
    "            if term == 'muslim':\n",
    "                print('HOW THE HELL')\n",
    "            synonyms = self.get_synonyms(term)\n",
    "            if len(synonyms) > 0:\n",
    "                if synonyms not in self.group_vocab.keys():\n",
    "                    self.group_vocab[synonyms] = 1\n",
    "                else:\n",
    "                    self.group_vocab[synonyms] += 1\n",
    "\n",
    "                for related_word in synonyms:\n",
    "                    related_word= related_word.lower()\n",
    "                    if related_word not in self.expanded_vocab.keys():\n",
    "                        self.expanded_vocab[related_word] = 1\n",
    "                    else:\n",
    "                        self.expanded_vocab[related_word] += 1\n",
    "\n",
    "                    if related_word not in self.term_group.keys():\n",
    "                        self.term_group[related_word] = synonyms\n",
    "\n",
    "                if synonyms not in self.group_docs[docId].keys():\n",
    "                    self.group_docs[docId][synonyms] = 1\n",
    "                else:\n",
    "                    self.group_docs[docId][synonyms] += 1\n",
    "\n",
    "                if synonyms in self.group_index.keys():\n",
    "                    self.group_index[synonyms].add(docId)\n",
    "                else:\n",
    "                    self.group_index[synonyms] = set()\n",
    "                    self.group_index[synonyms].add(docId)\n",
    "                \n",
    "        \n",
    "        if term not in self.vocab.keys():\n",
    "            self.vocab[term] = 1\n",
    "            for Id, docList in self.docs.items():\n",
    "                self.docs[Id][term] = 0\n",
    "        else:\n",
    "            self.vocab[term]+=1\n",
    "        \n",
    "        if term in self.docs[docId].keys():\n",
    "            self.docs[docId][term] += 1\n",
    "        else:\n",
    "            self.docs[docId][term] = 1\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        if term in self.index.keys():\n",
    "            self.index[term].add(docId)\n",
    "        else:\n",
    "            self.index[term] = set()\n",
    "            self.index[term].add(docId)\n",
    "    \n",
    "#         if docId not in self.occurrance.keys():\n",
    "#             self.occurrance[docId] = []\n",
    "#         self.occurrance[docId].append(position)\n",
    "\n",
    "    def get_query_group_vector(self, query_terms):\n",
    "        query_group_vector_hash = dict.fromkeys(self.group_vocab, 0)\n",
    "#         print(query_vector_hash)\n",
    "        \n",
    "        for term in query_terms:\n",
    "            term = lem.lemmatize(term.lower())\n",
    "            if term in self.term_group.keys():\n",
    "                synonyms = self.term_group[term]\n",
    "                if synonyms in query_group_vector_hash.keys():\n",
    "    #                 print('Getting Synonyms')\n",
    "    #                 print(term)\n",
    "    #                 print(synonyms)\n",
    "                    query_group_vector_hash[synonyms] += 1\n",
    "\n",
    "        \n",
    "            groups_in_query = len(query_terms)\n",
    "            tf = dict.fromkeys(self.group_vocab_idf, 0)\n",
    "            idf = dict.fromkeys(self.group_vocab_idf, 0)\n",
    "            tf_idf = dict.fromkeys(self.group_vocab_idf, 0)\n",
    "            \n",
    "            for syn_group,syn_group_cnt in query_group_vector_hash.items():\n",
    "                if syn_group_cnt <= 0:\n",
    "                    continue\n",
    "#                 print(syn_group)\n",
    "#                 print(syn_group_cnt)\n",
    "#                 if not isinstance(syn_group_cnt, int):\n",
    "#                     print(syn_group)\n",
    "#                     print(syn_group_cnt)\n",
    "#                 tf[term] = term_cnt / words_in_query\n",
    "#                 idf[term] = len(self.docs.keys()) / (self.vocab[term])\n",
    "                \n",
    "                tf[syn_group] = syn_group_cnt / groups_in_query\n",
    "                tf_idf[syn_group] = tf[syn_group] * self.group_vocab_idf[syn_group]\n",
    "        \n",
    "            new_query_vector_hash = {\n",
    "                'tf': tf,\n",
    "                'idf':self.group_vocab_idf,\n",
    "                'tf_idf':tf_idf\n",
    "            }\n",
    "    \n",
    "    \n",
    "        return new_query_vector_hash\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def get_query_vector(self, query_terms):\n",
    "        query_vector_hash = dict.fromkeys(self.vocab, 0)\n",
    "#         print(query_vector_hash)\n",
    "        for term in query_terms:\n",
    "            if term in query_vector_hash.keys():\n",
    "                query_vector_hash[term] += 1\n",
    "#             else:\n",
    "#                 query_vector_hash[term] = 1\n",
    "        \n",
    "            words_in_query = len(query_terms)\n",
    "            tf = {}\n",
    "            idf = {}\n",
    "            tf_idf = {}\n",
    "            \n",
    "            for term,term_cnt in query_vector_hash.items():\n",
    "#                 print(term)\n",
    "#                 print(term_cnt)\n",
    "                if not isinstance(term_cnt, int):\n",
    "                    print(term)\n",
    "                    print(term_cnt)\n",
    "#                 tf[term] = term_cnt / words_in_query\n",
    "#                 idf[term] = len(self.docs.keys()) / (self.vocab[term])\n",
    "                \n",
    "                tf[term] = term_cnt/ words_in_query\n",
    "                idf[term] = self.vocab_idf[term]\n",
    "                tf_idf[term] = tf[term] * idf[term]\n",
    "        \n",
    "            new_query_vector_hash = {\n",
    "                'tf': tf,\n",
    "                'idf':idf,\n",
    "                'tf_idf':tf_idf\n",
    "            }\n",
    "    \n",
    "    \n",
    "        return new_query_vector_hash\n",
    "    \n",
    "    def dot_product(self, v_hash_1, v_hash_2):\n",
    "#         print(len(v_hash_1))\n",
    "#         print(len(v_hash_2))\n",
    "#         print('vhash')\n",
    "#         print(v_hash_2)\n",
    "        return sum([v_hash_2[x]*y for x,y in v_hash_1.items()])\n",
    "        \n",
    "    def get_magnitude(self, v_hash):\n",
    "        mag = sum([x**2 for x in v_hash.values()]) ** 0.5 \n",
    "        if mag == 0:\n",
    "            print('Boy we got zero')\n",
    "            print([x for x in v_hash.values() if x > 0])\n",
    "            return 1\n",
    "        return mag\n",
    "    \n",
    "    \n",
    "    def get_cosine_sim(self, v_hash_x, v_hash_y):\n",
    "        \n",
    "        return (self.dot_product(v_hash_x,v_hash_y)) / (self.get_magnitude(v_hash_x) * self.get_magnitude(v_hash_y)) \n",
    "            \n",
    "    def get_ranking(self, query_vector_hash):\n",
    "        ranked_docs = []\n",
    "        for docId,doc_vector_hash in self.cdocs.items():\n",
    "            cosine_sim = self.get_cosine_sim(doc_vector_hash['tf_idf'], query_vector_hash['tf_idf'])\n",
    "            ranked_docs.append((docId, cosine_sim))\n",
    "        ranked_docs = sorted(ranked_docs, key=lambda x:x[1])\n",
    "        return ranked_docs\n",
    "    \n",
    "    def get_group_ranking(self, query_vector_hash):\n",
    "        ranked_docs = []\n",
    "        for docId,doc_vector_hash in self.group_cdocs.items():\n",
    "            cosine_sim = self.get_cosine_sim(doc_vector_hash['tf_idf'], query_vector_hash['tf_idf'])\n",
    "            ranked_docs.append((docId, cosine_sim))\n",
    "        ranked_docs = sorted(ranked_docs, key=lambda x:x[1])\n",
    "        return ranked_docs\n",
    "    \n",
    "    def calculate_tf_idf(self):\n",
    "        cdocs = {}\n",
    "        \n",
    "        self.vocab_idf = dict.fromkeys(self.vocab, 0)\n",
    "        self.group_vocab_idf = dict.fromkeys(self.group_vocab, 0)\n",
    "        \n",
    "        \n",
    "        for syn_group, group_cnt in self.group_vocab.items():\n",
    "            self.group_vocab_idf[syn_group] = math.log10((len(self.docs.keys()) / (len(self.group_index[syn_group])))+1)\n",
    "        \n",
    "        for term, term_cnt in self.vocab.items():\n",
    "            self.vocab_idf[term] = math.log10((len(self.docs.keys()) / (len(self.index[term]))) + 1 )\n",
    "            \n",
    "        for docId,doc in self.docs.items():\n",
    "#             print(doc.values())\n",
    "            words_in_d = sum(doc.values())\n",
    "            tf = {}\n",
    "        \n",
    "            tf_idf = {}\n",
    "            for term,term_cnt in doc.items():\n",
    "#                 tf[term] = term_cnt / words_in_d\n",
    "#                 idf[term] = len(self.docs.keys()) / (self.vocab[term])\n",
    "                tf[term] = term_cnt / words_in_d\n",
    "                tf_idf[term] = tf[term] * self.vocab_idf[term]\n",
    "            cdocs[docId] = {\n",
    "                'tf': tf,\n",
    "                'tf_idf':tf_idf,\n",
    "                'total_words' : words_in_d\n",
    "            }\n",
    "        group_cdocs = {}\n",
    "        for docId,doc in self.group_docs.items():\n",
    "#             print(doc.values())\n",
    "            groups_in_d = sum(doc.values())\n",
    "            tf = {}\n",
    "        \n",
    "            tf_idf = {}\n",
    "            for syn_group,syn_group_cnt in doc.items():\n",
    "#                 tf[term] = term_cnt / words_in_d\n",
    "#                 idf[term] = len(self.docs.keys()) / (self.vocab[term])\n",
    "                tf[syn_group] = syn_group_cnt / groups_in_d\n",
    "                tf_idf[syn_group] = tf[syn_group] * self.group_vocab_idf[syn_group]\n",
    "            group_cdocs[docId] = {\n",
    "                'tf': tf,\n",
    "                'tf_idf':tf_idf,\n",
    "                'total_words' : groups_in_d\n",
    "            }\n",
    "        self.cdocs = cdocs\n",
    "        self.group_cdocs = group_cdocs\n",
    "        return self.cdocs\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_space = VectorSpaceModel()\n",
    "\n",
    "d1= \"Music is a universal language\"\n",
    "d2= \"Music is a miracle\"\n",
    "d3= \"Music is a universal feature of the human experience\"\n",
    "test_space.create_doc(1)\n",
    "for word in d1.split(' '):\n",
    "    test_space.add_term(word, 1)\n",
    "test_space.create_doc(2)\n",
    "for word in d2.split(' '):\n",
    "    test_space.add_term(word, 2)\n",
    "test_space.create_doc(3)\n",
    "for word in d3.split(' '):\n",
    "    test_space.add_term(word, 3)\n",
    "# test_space.term_group\n",
    "test_space.expanded_vocab['music']\n",
    "test_space.expanded_vocab['euphony']\n",
    "# test_space.calculate_tf_idf()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Punctuation\n",
    "def remove_punctuation(word):\n",
    "    return word.translate(word.maketrans('','',string.punctuation))\n",
    "\n",
    "# Clean Query Term\n",
    "def clean_word(word):\n",
    "    # Case Folding\n",
    "    word = word.lower()\n",
    "     # Filter non-ASCII characters\n",
    "    word = ''.join(filter(lambda x: x in printable, word))\n",
    "#     print(word)\n",
    "    # Remove Punctuations\n",
    "    if word != '(' and word != ')':\n",
    "        word = remove_punctuation(word)\n",
    "#     print(word)\n",
    "    if re.match('\\d+[A-Za-z]+',word):\n",
    "        word = re.split('\\d+',word)[1]\n",
    "    if re.match('[A-Za-z]+\\d+',word):\n",
    "        word = re.split('\\d+',word)[0]\n",
    "#     print(word)\n",
    "    word = ps.stem(word)\n",
    "#     print(word)\n",
    "    return word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term - Document Indexing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading : ********************************************************Done\n"
     ]
    }
   ],
   "source": [
    "vocab = set()\n",
    "doc_contents = []\n",
    "inverted_index = InvertedIndex()\n",
    "vector_space = VectorSpaceModel()\n",
    "printable = set(string.printable) \n",
    "raw_data = []\n",
    "# Printable characters are\n",
    "# 0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\n",
    "# !\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~ \\t\\n\\r\\x0b\\x0c\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "lem = WordNetLemmatizer() \n",
    "\n",
    "stop_words = set()\n",
    "with open('Stopword-List.txt', 'r') as stop_word_file:\n",
    "    lines = stop_word_file.readlines()\n",
    "    for line in lines:\n",
    "        stop_words.add(line.split('\\n')[0])\n",
    "    stop_words.remove('')\n",
    "\n",
    "    \n",
    "print('Loading : ', end='')\n",
    "for file_number in range(0, 56):\n",
    "    vector_space.create_doc(file_number)\n",
    "    with open(f'data/Trump Speechs/speech_{file_number}.txt', 'r') as file1:\n",
    "        lines = file1.readlines()\n",
    "#         print(f'File Number : speech_{file_number}.txt' )\n",
    "#         print(lines[0])\n",
    "        position = {'doc':file_number,'row':0, 'col':0, 'token_no':0}\n",
    "        \n",
    "#         {\n",
    "#             'total_count' : 0,\n",
    "#             'postings' : {\n",
    "#                 'count':0,\n",
    "#                 'doc_id':0,\n",
    "#                 'positions':[]\n",
    "#             }\n",
    "#         }\n",
    "        \n",
    "        for line_no,line in enumerate(lines):\n",
    "            # Skip Heading Line\n",
    "            if line_no == 0:\n",
    "                continue\n",
    "            doc_set = set()\n",
    "            raw_data.append(line)\n",
    "            # split words at . , whitespace ? ! : ;\n",
    "            position['row'] = line_no \n",
    "            position['col'] = 0\n",
    "            symbols = \"!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~\\n\"\n",
    "            for i in symbols:\n",
    "                line = line.replace(i, ' ')\n",
    "            \n",
    "            for word in re.split('[.\\s,?!:;-]', line):\n",
    "                position['col'] += len(word) + 1\n",
    "                position['token_no'] += 1\n",
    "                # Case Folding\n",
    "                word = word.lower()\n",
    "                \n",
    "                # Filter non-ASCII characters\n",
    "                word = ''.join(filter(lambda x: x in printable, word))\n",
    "                \n",
    "                # Remove Punctuations\n",
    "                word = remove_punctuation(word)\n",
    "                \n",
    "                if re.match('\\d+[A-Za-z]+',word):\n",
    "                    word = re.split('\\d+',word)[1]\n",
    "                if re.match('[A-Za-z]+\\d+',word):\n",
    "                    word = re.split('\\d+',word)[0]\n",
    "                \n",
    "                if len(word) == 0 or len(word) == 1 or word == '' or word == ' ':\n",
    "                    continue\n",
    "                if word in stop_words:\n",
    "                    continue\n",
    "                \n",
    "                \n",
    "                \n",
    "                word = lem.lemmatize(word)\n",
    "    \n",
    "                    \n",
    "                vocab.add(word)\n",
    "                \n",
    "                doc_set.add(word)\n",
    "                \n",
    "                vector_space.add_term(word, file_number)\n",
    "                \n",
    "                \n",
    "                if word in inverted_index.index.keys():\n",
    "                    \n",
    "                    inverted_index.index[word].addOccurrance(file_number, copy.deepcopy(position)) \n",
    "                else:\n",
    "                    plist = PostingList()\n",
    "                    inverted_index.index[word] = plist\n",
    "                    inverted_index.index[word].addOccurrance(file_number, copy.deepcopy(position))\n",
    "                    \n",
    "        inverted_index.docs[file_number] = doc_set\n",
    "        doc_contents.append(doc_set)\n",
    "        print('*', end='')\n",
    "doc_term_tf_idf = vector_space.calculate_tf_idf()\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Vocabulary Size \n",
      "7014\n",
      "[[1 1 0 ... 6 0 0]\n",
      " [0 2 0 ... 1 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 2 0 ... 0 0 0]\n",
      " [0 3 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "  (0, 4136)\t1\n",
      "[[0 0 0 ... 0 0 0]]\n",
      "[[0 0 0 ... 0 0 0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TfidfTransformer(norm='l1', smooth_idf=True, sublinear_tf=False, use_idf=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00043717 0.00011168 0.         ... 0.00196056 0.         0.        ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(56, 7014)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.01269782, 0.02366651,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(raw_data)\n",
    "print('Total Vocabulary Size ')\n",
    "print(len(vectorizer.get_feature_names()))\n",
    "print(X.toarray())\n",
    "\n",
    "y = vectorizer.transform(['muslims'])\n",
    "print(y)\n",
    "print(y.toarray())\n",
    "print(y.todense())\n",
    "\n",
    "tfidf = TfidfTransformer()\n",
    "tfidf.fit(X)\n",
    "tf_idf_matrix = tfidf.transform(X)\n",
    "query_tf_idf = tfidf.transform(y)\n",
    "\n",
    "print(tf_idf_matrix.todense()[0])\n",
    "tf_idf_matrix.shape\n",
    "cosine_similarity(query_tf_idf, tf_idf_matrix)\n",
    "# print([y for x,y in sorted(vector_space.cdocs[0]['tf_idf'].items(), key = lambda x: x[0]) ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Vocabulary Size \n",
      "6249\n",
      "Total Number of Documents \n",
      "56\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'clinton'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Total Vocabulary Size ')\n",
    "print(len(inverted_index.index.keys()))\n",
    "print('Total Number of Documents ')\n",
    "print(len(inverted_index.docs))\n",
    "# print('Occurances of query: hammer')\n",
    "# inverted_index.index[lem.lemmatize('interest')].occurrance\n",
    "# import nltk\n",
    "# nltk.download('wordnet')\n",
    "# vector_space.index[lem.lemmatize('hillary')]\n",
    "lem.lemmatize('clinton')\n",
    "# vector_space.cdocs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hillary', 'clinton']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Hillary clinton\"\n",
    "query_terms = [lem.lemmatize(x) for x in query.split(' ')]\n",
    "print(query_terms)\n",
    "\n",
    "query_vector = vector_space.get_query_vector(query_terms)\n",
    "\n",
    "# cs = vector_space.get_cosine_sim(vector_space.docs[1], query_vector)\n",
    "# print([(x,y) for x,y in query_vector.items() if y > 0])\n",
    "# ranked = vector_space.get_ranking(vector_space.get_query_vector(query_terms))\n",
    "# ranked.reverse()\n",
    "# ranked = set([x for x,y in ranked if y > 0.0005])\n",
    "# print(ranked)\n",
    "\n",
    "vector_space.cdocs[41]['tf'][lem.lemmatize('global')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t['massive', 'inflow', 'of', 'refugee']\n",
      "Normal results\n",
      "results length :  56\n",
      "Result Differnece : \n",
      "set()\n",
      "Symmetric Difference : \n",
      "{'33', '27', '16', '23', '7', '22', '6', '55', '25', '18', '4', '28', '35', '0', '5', '42', '19', '3', '24', '10', '1', '15', '21', '43', '9', '45', '2', '17', '13', '8', '36', '14', '34', '11', '26', '53', '51'}\n",
      "\n",
      "precision : 0.3392857142857143\n",
      "recall : 1.0\n",
      "Sciket results\n",
      "results length :  56\n",
      "Result Differnece : \n",
      "set()\n",
      "Symmetric Difference : \n",
      "{'33', '27', '16', '23', '7', '6', '22', '55', '25', '18', '4', '28', '35', '0', '5', '19', '42', '3', '10', '24', '51', '1', '15', '21', '43', '9', '45', '2', '17', '13', '36', '14', '34', '11', '26', '53', '8'}\n",
      "\n",
      "\n",
      "precision : 0.3392857142857143\n",
      "recall : 1.0\n",
      "Group results\n",
      "results length :  56\n",
      "Result Differnece : \n",
      "set()\n",
      "Symmetric Difference : \n",
      "{'33', '27', '16', '23', '7', '22', '6', '55', '25', '18', '4', '28', '35', '0', '5', '42', '19', '3', '24', '10', '1', '15', '21', '43', '9', '45', '2', '17', '13', '8', '36', '14', '34', '11', '26', '53', '51'}\n",
      "\n",
      "precision : 0.3392857142857143\n",
      "recall : 1.0\n",
      "\t['pakistan', 'afghanistan']\n",
      "Normal results\n",
      "results length :  12\n",
      "Result Differnece : \n",
      "set()\n",
      "Symmetric Difference : \n",
      "{'41', '37', '39', '9', '18', '40'}\n",
      "\n",
      "precision : 0.5\n",
      "recall : 1.0\n",
      "Sciket results\n",
      "results length :  12\n",
      "Result Differnece : \n",
      "set()\n",
      "Symmetric Difference : \n",
      "{'40', '41', '9', '39', '18', '37'}\n",
      "\n",
      "\n",
      "precision : 0.5\n",
      "recall : 1.0\n",
      "Group results\n",
      "results length :  12\n",
      "Result Differnece : \n",
      "set()\n",
      "Symmetric Difference : \n",
      "{'41', '37', '39', '9', '18', '40'}\n",
      "\n",
      "precision : 0.5\n",
      "recall : 1.0\n",
      "\t['hillary', 'clinton']\n",
      "Normal results\n",
      "results length :  53\n",
      "Result Differnece : \n",
      "set()\n",
      "Symmetric Difference : \n",
      "{'0'}\n",
      "\n",
      "precision : 0.9811320754716981\n",
      "recall : 1.0\n",
      "Sciket results\n",
      "results length :  53\n",
      "Result Differnece : \n",
      "set()\n",
      "Symmetric Difference : \n",
      "{'0'}\n",
      "\n",
      "\n",
      "precision : 0.9811320754716981\n",
      "recall : 1.0\n",
      "Group results\n",
      "results length :  53\n",
      "Result Differnece : \n",
      "set()\n",
      "Symmetric Difference : \n",
      "{'0'}\n",
      "\n",
      "precision : 0.9811320754716981\n",
      "recall : 1.0\n",
      "\t['personnel', 'policy']\n",
      "Normal results\n",
      "results length :  43\n",
      "Result Differnece : \n",
      "set()\n",
      "Symmetric Difference : \n",
      "{'33', '23', '16', '7', '6', '39', '48', '4', '32', '37', '31', '12', '49', '42', '19', '3', '20', '24', '52', '44', '21', '43', '9', '45', '2', '13', '8', '54', '36', '14', '34', '26', '40', '51'}\n",
      "\n",
      "precision : 0.20930232558139536\n",
      "recall : 1.0\n",
      "Sciket results\n",
      "results length :  37\n",
      "Result Differnece : \n",
      "set()\n",
      "Symmetric Difference : \n",
      "{'33', '16', '23', '7', '39', '4', '32', '37', '31', '12', '19', '42', '3', '20', '24', '52', '44', '43', '9', '45', '2', '36', '54', '14', '34', '26', '40', '51'}\n",
      "\n",
      "\n",
      "precision : 0.24324324324324326\n",
      "recall : 1.0\n",
      "Group results\n",
      "results length :  45\n",
      "Result Differnece : \n",
      "set()\n",
      "Symmetric Difference : \n",
      "{'33', '23', '16', '7', '6', '39', '48', '4', '32', '37', '31', '12', '49', '42', '19', '3', '20', '24', '52', '44', '41', '21', '43', '9', '45', '30', '2', '13', '8', '54', '36', '14', '34', '26', '40', '51'}\n",
      "\n",
      "precision : 0.2\n",
      "recall : 1.0\n",
      "\t['united', 'plane']\n",
      "Normal results\n",
      "results length :  51\n",
      "Result Differnece : \n",
      "set()\n",
      "Symmetric Difference : \n",
      "{'6', '0'}\n",
      "\n",
      "precision : 0.9607843137254902\n",
      "recall : 1.0\n",
      "Sciket results\n",
      "results length :  51\n",
      "Result Differnece : \n",
      "set()\n",
      "Symmetric Difference : \n",
      "{'6', '0'}\n",
      "\n",
      "\n",
      "precision : 0.9607843137254902\n",
      "recall : 1.0\n",
      "Group results\n",
      "results length :  53\n",
      "Result Differnece : \n",
      "set()\n",
      "Symmetric Difference : \n",
      "{'23', '6', '0', '42'}\n",
      "\n",
      "precision : 0.9245283018867925\n",
      "recall : 1.0\n",
      "\t['develop', 'solution']\n",
      "Normal results\n",
      "results length :  25\n",
      "Result Differnece : \n",
      "set()\n",
      "Symmetric Difference : \n",
      "{'25', '31', '1', '8'}\n",
      "\n",
      "precision : 0.84\n",
      "recall : 1.0\n",
      "Sciket results\n",
      "results length :  23\n",
      "Result Differnece : \n",
      "set()\n",
      "Symmetric Difference : \n",
      "{'31', '8'}\n",
      "\n",
      "\n",
      "precision : 0.9130434782608695\n",
      "recall : 1.0\n",
      "Group results\n",
      "results length :  56\n",
      "Result Differnece : \n",
      "set()\n",
      "Symmetric Difference : \n",
      "{'7', '55', '22', '6', '25', '48', '46', '4', '28', '37', '31', '12', '49', '0', '42', '19', '10', '1', '44', '15', '50', '41', '43', '29', '45', '13', '8', '54', '36', '14', '34', '47', '26', '11', '53'}\n",
      "\n",
      "precision : 0.375\n",
      "recall : 1.0\n",
      "\t['development', 'praised']\n",
      "Normal results\n",
      "results length :  4\n",
      "Result Differnece : \n",
      "set()\n",
      "Symmetric Difference : \n",
      "{'31'}\n",
      "\n",
      "precision : 0.75\n",
      "recall : 1.0\n",
      "Sciket results\n",
      "results length :  3\n",
      "Result Differnece : \n",
      "set()\n",
      "Symmetric Difference : \n",
      "set()\n",
      "\n",
      "\n",
      "precision : 1.0\n",
      "recall : 1.0\n",
      "Group results\n",
      "results length :  18\n",
      "Result Differnece : \n",
      "set()\n",
      "Symmetric Difference : \n",
      "{'33', '50', '23', '7', '48', '46', '13', '31', '38', '49', '47', '42', '3', '51', '24'}\n",
      "\n",
      "precision : 0.16666666666666666\n",
      "recall : 1.0\n",
      "\t['muslim']\n",
      "Normal results\n",
      "results length :  7\n",
      "Result Differnece : \n",
      "set()\n",
      "Symmetric Difference : \n",
      "{'2', '7', '6', '20', '9'}\n",
      "\n",
      "precision : 0.2857142857142857\n",
      "recall : 1.0\n",
      "Sciket results\n",
      "results length :  2\n",
      "Result Differnece : \n",
      "set()\n",
      "Symmetric Difference : \n",
      "set()\n",
      "\n",
      "\n",
      "precision : 1.0\n",
      "recall : 1.0\n",
      "Group results\n",
      "results length :  36\n",
      "Result Differnece : \n",
      "set()\n",
      "Symmetric Difference : \n",
      "{'33', '27', '16', '7', '6', '22', '48', '46', '18', '32', '35', '49', '0', '19', '10', '20', '24', '1', '50', '41', '21', '29', '9', '30', '2', '17', '13', '8', '54', '36', '34', '47', '11', '51'}\n",
      "\n",
      "precision : 0.05555555555555555\n",
      "recall : 1.0\n",
      "\t['american', 'energy', 'revolution']\n",
      "Normal results\n",
      "results length :  55\n",
      "Result Differnece : \n",
      "set()\n",
      "Symmetric Difference : \n",
      "{'6', '0'}\n",
      "\n",
      "precision : 0.9636363636363636\n",
      "recall : 1.0\n",
      "Sciket results\n",
      "results length :  53\n",
      "Result Differnece : \n",
      "set()\n",
      "Symmetric Difference : \n",
      "set()\n",
      "\n",
      "\n",
      "precision : 1.0\n",
      "recall : 1.0\n",
      "Group results\n",
      "results length :  55\n",
      "Result Differnece : \n",
      "set()\n",
      "Symmetric Difference : \n",
      "{'6', '0'}\n",
      "\n",
      "precision : 0.9636363636363636\n",
      "recall : 1.0\n",
      "\t['future', 'of', 'new', 'america']\n",
      "Normal results\n",
      "results length :  56\n",
      "Result Differnece : \n",
      "set()\n",
      "Symmetric Difference : \n",
      "{'28', '0', '53'}\n",
      "\n",
      "precision : 0.9464285714285714\n",
      "recall : 1.0\n",
      "Sciket results\n",
      "results length :  56\n",
      "Result Differnece : \n",
      "set()\n",
      "Symmetric Difference : \n",
      "{'28', '0', '53'}\n",
      "\n",
      "\n",
      "precision : 0.9464285714285714\n",
      "recall : 1.0\n",
      "Group results\n",
      "results length :  56\n",
      "Result Differnece : \n",
      "set()\n",
      "Symmetric Difference : \n",
      "{'28', '0', '53'}\n",
      "\n",
      "precision : 0.9464285714285714\n",
      "recall : 1.0\n",
      "\t['hillary', 'clinton', 'is', 'the', 'worst', 'looser']\n",
      "Normal results\n",
      "results length :  56\n",
      "Result Differnece : \n",
      "set()\n",
      "Symmetric Difference : \n",
      "{'23', '0', '15', '13'}\n",
      "\n",
      "precision : 0.9285714285714286\n",
      "recall : 1.0\n",
      "Sciket results\n",
      "results length :  56\n",
      "Result Differnece : \n",
      "set()\n",
      "Symmetric Difference : \n",
      "{'23', '0', '15', '13'}\n",
      "\n",
      "\n",
      "precision : 0.9285714285714286\n",
      "recall : 1.0\n",
      "Group results\n",
      "results length :  56\n",
      "Result Differnece : \n",
      "set()\n",
      "Symmetric Difference : \n",
      "{'23', '0', '15', '13'}\n",
      "\n",
      "precision : 0.9285714285714286\n",
      "recall : 1.0\n",
      "\t['no', 'patience', 'for', 'injustice']\n",
      "Normal results\n",
      "results length :  5\n",
      "Result Differnece : \n",
      "set()\n",
      "Symmetric Difference : \n",
      "set()\n",
      "\n",
      "precision : 1.0\n",
      "recall : 1.0\n",
      "Sciket results\n",
      "results length :  56\n",
      "Result Differnece : \n",
      "set()\n",
      "Symmetric Difference : \n",
      "{'33', '27', '23', '6', '55', '25', '39', '48', '18', '46', '4', '28', '32', '37', '35', '31', '38', '12', '49', '0', '5', '19', '42', '3', '10', '20', '24', '51', '1', '52', '44', '50', '41', '21', '43', '29', '9', '45', '30', '2', '17', '13', '36', '54', '14', '34', '47', '26', '40', '53', '8'}\n",
      "\n",
      "\n",
      "precision : 0.08928571428571429\n",
      "recall : 1.0\n",
      "Group results\n",
      "results length :  8\n",
      "Result Differnece : \n",
      "set()\n",
      "Symmetric Difference : \n",
      "{'43', '4', '37'}\n",
      "\n",
      "precision : 0.625\n",
      "recall : 1.0\n",
      "\t['global', 'interest']\n",
      "Normal results\n",
      "results length :  48\n",
      "Result Differnece : \n",
      "{'27', '41', '39', '47', '24'}\n",
      "Symmetric Difference : \n",
      "{'33', '27', '23', '7', '6', '55', '39', '46', '4', '32', '28', '37', '35', '31', '38', '12', '49', '0', '5', '19', '3', '20', '52', '24', '1', '44', '50', '41', '43', '9', '45', '30', '2', '17', '36', '34', '14', '47', '26', '53', '51'}\n",
      "\n",
      "precision : 0.32075471698113206\n",
      "recall : 0.7727272727272727\n",
      "Sciket results\n",
      "results length :  48\n",
      "Result Differnece : \n",
      "{'27', '41', '39', '47', '24'}\n",
      "Symmetric Difference : \n",
      "{'33', '27', '23', '7', '6', '55', '39', '46', '4', '28', '32', '37', '35', '31', '38', '12', '49', '0', '5', '19', '3', '20', '51', '52', '1', '24', '44', '50', '41', '43', '9', '45', '30', '2', '17', '36', '14', '34', '47', '26', '53'}\n",
      "\n",
      "\n",
      "precision : 0.32075471698113206\n",
      "recall : 0.7727272727272727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group results\n",
      "results length :  56\n",
      "Result Differnece : \n",
      "set()\n",
      "Symmetric Difference : \n",
      "{'33', '23', '7', '55', '6', '46', '4', '32', '28', '37', '35', '31', '38', '12', '49', '0', '5', '19', '3', '20', '52', '1', '44', '15', '50', '43', '29', '9', '45', '30', '2', '17', '13', '36', '34', '14', '26', '53', '51'}\n",
      "\n",
      "precision : 0.30357142857142855\n",
      "recall : 1.0\n",
      "\t['pakistan', 'afghanistan', 'aid']\n",
      "Normal results\n",
      "results length :  14\n",
      "Result Differnece : \n",
      "set()\n",
      "Symmetric Difference : \n",
      "{'37', '9', '18'}\n",
      "\n",
      "precision : 0.7857142857142857\n",
      "recall : 1.0\n",
      "Sciket results\n",
      "results length :  14\n",
      "Result Differnece : \n",
      "set()\n",
      "Symmetric Difference : \n",
      "{'9', '18', '37'}\n",
      "\n",
      "\n",
      "precision : 0.7857142857142857\n",
      "recall : 1.0\n",
      "Group results\n",
      "results length :  51\n",
      "Result Differnece : \n",
      "set()\n",
      "Symmetric Difference : \n",
      "{'33', '27', '23', '55', '6', '25', '48', '18', '46', '28', '32', '37', '35', '31', '38', '49', '0', '5', '19', '10', '52', '24', '15', '44', '50', '21', '43', '9', '45', '30', '2', '13', '8', '54', '36', '14', '34', '47', '11', '51'}\n",
      "\n",
      "precision : 0.21568627450980393\n",
      "recall : 1.0\n",
      "\t['biggest', 'plane', 'wanted', 'hour']\n",
      "Normal results\n",
      "results length :  47\n",
      "Result Differnece : \n",
      "set()\n",
      "Symmetric Difference : \n",
      "{'33', '27', '16', '7', '6', '55', '25', '39', '48', '46', '18', '4', '28', '32', '37', '35', '49', '0', '5', '42', '3', '10', '24', '15', '44', '50', '41', '43', '29', '9', '45', '30', '2', '17', '8', '36', '54', '34', '47', '11', '26', '40', '53', '51'}\n",
      "\n",
      "precision : 0.06382978723404255\n",
      "recall : 1.0\n",
      "Sciket results\n",
      "results length :  35\n",
      "Result Differnece : \n",
      "set()\n",
      "Symmetric Difference : \n",
      "{'16', '7', '6', '48', '39', '18', '46', '4', '37', '35', '49', '0', '42', '3', '51', '44', '50', '41', '43', '9', '45', '30', '2', '17', '36', '54', '47', '11', '26', '40', '53', '8'}\n",
      "\n",
      "\n",
      "precision : 0.08571428571428572\n",
      "recall : 1.0\n",
      "Group results\n",
      "results length :  54\n",
      "Result Differnece : \n",
      "set()\n",
      "Symmetric Difference : \n",
      "{'33', '27', '16', '23', '7', '6', '55', '22', '25', '39', '48', '46', '18', '4', '32', '28', '37', '35', '31', '12', '49', '0', '5', '42', '3', '20', '24', '10', '44', '15', '50', '41', '21', '43', '29', '9', '45', '30', '2', '17', '8', '36', '54', '34', '14', '47', '11', '26', '40', '53', '51'}\n",
      "\n",
      "precision : 0.05555555555555555\n",
      "recall : 1.0\n",
      "\t['near', 'architect', 'box']\n",
      "Normal results\n",
      "results length :  19\n",
      "Result Differnece : \n",
      "set()\n",
      "Symmetric Difference : \n",
      "{'44', '50', '6', '25', '46', '45', '47', '53'}\n",
      "\n",
      "precision : 0.5789473684210527\n",
      "recall : 1.0\n",
      "Sciket results\n",
      "results length :  12\n",
      "Result Differnece : \n",
      "set()\n",
      "Symmetric Difference : \n",
      "{'6'}\n",
      "\n",
      "\n",
      "precision : 0.9166666666666666\n",
      "recall : 1.0\n",
      "Group results\n",
      "results length :  53\n",
      "Result Differnece : \n",
      "set()\n",
      "Symmetric Difference : \n",
      "{'33', '27', '16', '7', '6', '22', '48', '25', '46', '28', '37', '35', '31', '38', '12', '49', '0', '5', '19', '42', '3', '10', '20', '52', '1', '44', '15', '50', '41', '21', '29', '45', '30', '2', '8', '36', '14', '34', '47', '26', '40', '53'}\n",
      "\n",
      "precision : 0.20754716981132076\n",
      "recall : 1.0\n",
      "\t['peaceful', 'change']\n",
      "Normal results\n",
      "results length :  49\n",
      "Result Differnece : \n",
      "set()\n",
      "Symmetric Difference : \n",
      "{'6', '55', '18', '19', '15', '9', '34', '26', '40'}\n",
      "\n",
      "precision : 0.8163265306122449\n",
      "recall : 1.0\n",
      "Sciket results\n",
      "results length :  48\n",
      "Result Differnece : \n",
      "set()\n",
      "Symmetric Difference : \n",
      "{'6', '18', '19', '15', '9', '34', '26', '40'}\n",
      "\n",
      "\n",
      "precision : 0.8333333333333334\n",
      "recall : 1.0\n",
      "Group results\n",
      "results length :  50\n",
      "Result Differnece : \n",
      "set()\n",
      "Symmetric Difference : \n",
      "{'6', '55', '18', '0', '19', '15', '9', '34', '26', '40'}\n",
      "\n",
      "precision : 0.8\n",
      "recall : 1.0\n",
      "normal f1 : 0.04664975477747482\n",
      "sciket f1 : 0.04804114680768719\n",
      "group f1 : 0.03948592488841693\n"
     ]
    }
   ],
   "source": [
    "test = [(None, None) for x in range(0, 17)] \n",
    "test[0] =(\"massive inflow of refugees\",\n",
    "{'32', '50', '49', '47', '46', '29', '48', '54', '41', '40', '30', '39', '12', '52', '37', '44', '31', '38', '20'}\n",
    "         )\n",
    "test[1] =('pakistan afghanistan',\n",
    "{'3', '22', '16', '17', '4', '1'}\n",
    "         )\n",
    "test[2] =('Hillary Clinton',\n",
    "{'4', '12', '36', '42', '33', '20', '29', '35', '16', '22', '47', '34', '14', '21', '45', '46', '41', '11', '3', '51', '53', '40', '37', '17', '48', '39', '5', '28', '10', '18', '54', '44', '25', '43', '49', '24', '30', '32', '8', '50', '9', '26', '7', '19', '52', '31', '27', '6', '2', '38', '1', '55'}\n",
    "         )\n",
    "test[3] =('personnel policies',\n",
    "\n",
    "{'5', '18', '11', '25', '10', '22', '29', '27', '17'}\n",
    "         )\n",
    "\n",
    "test[4] =('united plane',\n",
    "{'38', '1', '25', '41', '48', '29', '3', '17', '49', '20', '4', '52', '13', '39', '46', '22', '40', '36', '28', '12', '45', '26', '2', '51', '50', '19', '24', '5', '47', '31', '35', '21', '37', '55', '9', '33', '44', '54', '34', '7', '32', '43', '30', '16', '27', '18', '11', '10', '8'}\n",
    "         )\n",
    "\n",
    "test[5] =('develop solutions',\n",
    "{'38', '23', '2', '17', '32', '18', '51', '16', '39', '35', '21', '27', '33', '20', '24', '5', '3', '9', '40', '52', '30'}\n",
    "         )\n",
    "\n",
    "test[6] =('developments praised',\n",
    "{'36', '44', '8'}\n",
    "         )\n",
    "test[7] =(\"muslims\",\n",
    "{'4', '3'}\n",
    "         )\n",
    "test[8] =('American Energy Revolution',\n",
    "\n",
    "{'13', '28', '35', '33', '51', '27', '12', '36', '34', '14', '21', '29', '31', '30', '25', '32', '26', '24', '11', '54', '49', '16', '46', '48', '10', '18', '4', '50', '42', '5', '23', '47', '20', '43', '37', '39', '41', '45', '2', '22', '3', '40', '7', '17', '52', '44', '53', '19', '9', '55', '1', '15', '8'}\n",
    "         )\n",
    "test[9] =('Future of new America',\n",
    "{'51', '14', '35', '26', '13', '50', '33', '4', '12', '31', '20', '32', '29', '34', '41', '27', '46', '30', '40', '25', '17', '42', '16', '24', '54', '2', '43', '21', '7', '18', '52', '49', '5', '47', '45', '10', '36', '38', '44', '11', '3', '39', '22', '9', '48', '37', '23', '15', '1', '19', '55', '6', '8'}\n",
    "         )\n",
    "test[10] =('Hillary clinton is the worst looser',\n",
    "{'4', '12', '36', '42', '33', '20', '29', '35', '16', '22', '47', '34', '14', '21', '45', '46', '41', '11', '3', '51', '53', '40', '37', '17', '48', '39', '5', '28', '10', '18', '54', '44', '25', '43', '49', '24', '30', '32', '8', '50', '9', '26', '7', '19', '52', '31', '27', '6', '2', '38', '1', '55'}\n",
    "          )\n",
    "test[11] =('no patience for injustice',\n",
    "{'11', '22', '16', '7', '15'}\n",
    "          )\n",
    "test[12] =('Global interests',\n",
    "{'10', '16', '27', '42', '21', '11', '25', '22', '47', '41', '48', '54', '18', '40', '24', '39', '8'}\n",
    "          )\n",
    "test[13] =('pakistan afghanistan aid',\n",
    "{'3', '22', '42', '29', '41', '16', '40', '39', '17', '4', '1'}\n",
    "          )\n",
    "test[14] =('biggest plane wanted hour',\n",
    "{'52', '1', '19'}\n",
    "          )\n",
    "test[15] =('near architect box',\n",
    "{'54', '23', '18', '24', '39', '43', '51', '4', '9', '17', '11'}\n",
    "          )\n",
    "test[16] =('peaceful change',\n",
    "{'14', '10', '4', '30', '3', '11', '20', '16', '50', '52', '48', '17', '7', '2', '22', '29', '13', '53', '41', '46', '32', '21', '39', '25', '33', '54', '42', '5', '36', '31', '43', '51', '49', '23', '47', '35', '37', '12', '45', '8'}\n",
    "         )\n",
    "\n",
    "cumulative_precision_normal = 0\n",
    "cumulative_recall_normal = 0 \n",
    "cumulative_precision_sciket = 0\n",
    "cumulative_recall_sciket = 0 \n",
    "cumulative_precision_group = 0\n",
    "cumulative_recall_group = 0 \n",
    "\n",
    "#         tp, fp, tn\n",
    "sciket_results = []\n",
    "cnt = 0\n",
    "for t in test:\n",
    "    cnt+=1\n",
    "    normal_cnt = [0, 0 , 0]\n",
    "    sciket_cnt = [0, 0, 0]\n",
    "    group_cnt = [0, 0, 0]\n",
    "    \n",
    "    # sciket model\n",
    "    y = vectorizer.transform([t[0]])\n",
    "    query_tf_idf = tfidf.transform(y)\n",
    "    sciket_cosines = cosine_similarity(query_tf_idf, tf_idf_matrix)[0]\n",
    "    # my custom model\n",
    "    query = t[0]\n",
    "    query_terms = [lem.lemmatize(x.lower()) for x in query.split(' ')]\n",
    "    print('\\t' + str(query_terms))\n",
    "\n",
    "    query_vector = vector_space.get_query_vector(query_terms)\n",
    "    query_group_vector = vector_space.get_query_group_vector(query_terms)\n",
    "    # cs = vector_space.get_cosine_sim(vector_space.docs[1], query_vector)\n",
    "    # print([(x,y) for x,y in query_vector.items() if y > 0])\n",
    "    ranked = vector_space.get_ranking(vector_space.get_query_vector(query_terms))\n",
    "    ranked.reverse()\n",
    "    ranked = set([str(x) for x,y in ranked if y > 0.0005])\n",
    "#     print(ranked)\n",
    "    print('Normal results')\n",
    "    print(f'results length :  {len(ranked)}')\n",
    "    print('Result Differnece : ')\n",
    "    print(t[1].difference(ranked))\n",
    "    print('Symmetric Difference : ')\n",
    "    print(t[1].symmetric_difference(ranked))\n",
    "    print()\n",
    "    normal_cnt = [len(t[1]), len(ranked.difference(t[1])), len(t[1].difference(ranked))]\n",
    "    normal_p = normal_cnt[0] / (normal_cnt[0] + normal_cnt[1])\n",
    "    normal_r = normal_cnt[0] / (normal_cnt[0] + normal_cnt[2])\n",
    "    print(f'precision : {normal_p}')\n",
    "    print(f'recall : {normal_r}')\n",
    "    cumulative_precision_normal += normal_p\n",
    "    cumulative_recall_normal += normal_r\n",
    "    \n",
    "#     Sciket results\n",
    "    print('Sciket results')\n",
    "    sciket_ranked = [str(index) for index, cosine in enumerate(sciket_cosines) if cosine > 0.0005]\n",
    "    print(f'results length :  {len(sciket_ranked)}')\n",
    "    print('Result Differnece : ')\n",
    "    print(t[1].difference(set(sciket_ranked)))\n",
    "    print('Symmetric Difference : ')\n",
    "    print(t[1].symmetric_difference(set(sciket_ranked)))\n",
    "    print()\n",
    "    sciket_cnt = [len(t[1]), len(set(sciket_ranked).difference(t[1])), len(t[1].difference(set(sciket_ranked)))]\n",
    "    sciket_p = sciket_cnt[0] / (sciket_cnt[0] + sciket_cnt[1])\n",
    "    sciket_r = sciket_cnt[0] / (sciket_cnt[0] + sciket_cnt[2])\n",
    "    print()\n",
    "    print(f'precision : {sciket_p}')\n",
    "    print(f'recall : {sciket_r}')\n",
    "    cumulative_precision_sciket += sciket_p\n",
    "    cumulative_recall_sciket += sciket_r\n",
    "# Group Results \n",
    "    group_ranked = vector_space.get_group_ranking(query_group_vector)\n",
    "    group_ranked.reverse()\n",
    "    group_ranked = set([str(x) for x,y in group_ranked if y > 0.0005])\n",
    "    \n",
    "    print('Group results')\n",
    "    print(f'results length :  {len(group_ranked)}')\n",
    "    print('Result Differnece : ')\n",
    "    print(t[1].difference(set(group_ranked)))\n",
    "    print('Symmetric Difference : ')\n",
    "    print(t[1].symmetric_difference(set(group_ranked)))\n",
    "    print()\n",
    "    group_cnt = [len(t[1]), len(group_ranked.difference(t[1])), len(t[1].difference(group_ranked))]\n",
    "    group_p = group_cnt[0] / (group_cnt[0] + group_cnt[1])\n",
    "    group_r = group_cnt[0] / (group_cnt[0] + group_cnt[2])\n",
    "    print(f'precision : {group_p}')\n",
    "    print(f'recall : {group_r}')\n",
    "    cumulative_precision_group += group_p\n",
    "    cumulative_recall_group += group_r\n",
    "    \n",
    "f1_normal = (2 * (cumulative_precision_normal / cnt) * (cumulative_recall_normal / cnt) ) / (cumulative_recall_normal+cumulative_precision_normal)\n",
    "f1_sciket = (2 * (cumulative_precision_sciket / cnt) * (cumulative_recall_sciket / cnt) ) / (cumulative_recall_sciket+cumulative_precision_sciket)\n",
    "f1_group = (2 * (cumulative_precision_group / cnt) * (cumulative_recall_group / cnt) ) / (cumulative_recall_group+cumulative_precision_group)\n",
    "\n",
    "print(f'normal f1 : {f1_normal}')\n",
    "print(f'sciket f1 : {f1_sciket}')\n",
    "print(f'group f1 : {f1_group}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To save Inverted Index in File\n",
    "with open('pickled/inverted_index.p', 'wb') as index_file:\n",
    "    pickle.dump(inverted_index, index_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To Load Inverted Index from File\n",
    "with open('pickled/inverted_index.p', 'rb') as index_file:\n",
    "    inverted_index = pickle.load(index_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersect_posting(p1, p2):\n",
    "    if len(p1) == 0 or len(p2) == 0:\n",
    "        return PostingList()\n",
    "\n",
    "    if isinstance(p1, set) and isinstance(p2, set):\n",
    "        return p1.intersection(p2)\n",
    "    elif isinstance(p1, set):\n",
    "        return p1.intersection(p2.occurrance.keys())\n",
    "    elif isinstance(p2, set):\n",
    "        return p2.intersection(p1.occurrance.keys())\n",
    "    pn = PostingList()\n",
    "    # pn.token = f'{p1.token} & {p2.token}'\n",
    "    for pn_keys in (p1.occurrance.keys() & p2.occurrance.keys()) :\n",
    "        pn.addOccurrance(pn_keys, p1.occurrance[pn_keys])\n",
    "        pn.addOccurrance(pn_keys, p2.occurrance[pn_keys])\n",
    "    return pn\n",
    "\n",
    "\n",
    "def union_posting(p1, p2):\n",
    "    if len(p1) == 0:\n",
    "        return p2\n",
    "    elif len(p2) == 0:\n",
    "        return p1\n",
    "    if isinstance(p1, set) and isinstance(p2, set):\n",
    "        return p1.union(p2)\n",
    "    elif isinstance(p1, set):\n",
    "        return p1.union(p2.occurrance.keys())\n",
    "    elif isinstance(p2, set):\n",
    "        return p2.union(p1.occurance.keys())\n",
    "    \n",
    "    pn = PostingList()\n",
    "    # pn.token = f'{p1.token} | {p2.token}'\n",
    "    for pn1_keys in p1.occurrance.keys() :\n",
    "        pn.addOccurrance(pn1_keys, p1.occurrance[pn1_keys])\n",
    "    for pn2_keys in p2.occurrance.keys() :\n",
    "        pn.addOccurrance(pn2_keys, p2.occurrance[pn2_keys])\n",
    "    \n",
    "    return pn\n",
    "\n",
    "def inverse_posting(inverted_index,p):\n",
    "    print(p)\n",
    "    if isinstance(p, set) :\n",
    "        print('Returning ')\n",
    "        print(set(inverted_index.docs).difference(p))\n",
    "        return set(inverted_index.docs).difference(p)\n",
    "    else:\n",
    "        print(set(inverted_index.docs).difference(set(p.occurrance.keys())))\n",
    "        return set(inverted_index.docs).difference(set(p.occurrance.keys()))\n",
    "    return inverted_index.docs.keys() - p.occurrance.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "def build_index():\n",
    "    path_to_data = os.path.dirname(__file__) + '../../data/'\n",
    "    print(os.path.dirname(__file__))\n",
    "    print(path_to_data)\n",
    "    vocab = set()\n",
    "    doc_contents = []\n",
    "    inverted_index = InvertedIndex()\n",
    "    printable = set(string.printable) \n",
    "    # Printable characters are\n",
    "    # 0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\n",
    "    # !\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~ \\t\\n\\r\\x0b\\x0c\n",
    "\n",
    "\n",
    "    ps = PorterStemmer()\n",
    "    stop_words = set()\n",
    "    with open(path_to_data+'Stopword-List.txt', 'r') as stop_word_file:\n",
    "        lines = stop_word_file.readlines()\n",
    "        for line in lines:\n",
    "            stop_words.add(line.split('\\n')[0])\n",
    "        stop_words.remove('')\n",
    "    print(stop_words)\n",
    "\n",
    "    for file_number in range(0, 56):\n",
    "        with open(path_to_data + f'Trump Speechs/speech_{file_number}.txt', 'r') as file1:\n",
    "            lines = file1.readlines()\n",
    "            print(f'File Number : speech_{file_number}.txt' )\n",
    "            print(lines[0])\n",
    "            position = {'doc':file_number,'row':0, 'col':0, 'token_no':0}\n",
    "\n",
    "            for line_no,line in enumerate(lines):\n",
    "                doc_set = set()\n",
    "                # split words at . , whitespace ? ! : ;\n",
    "                position['row'] = line_no \n",
    "                position['col'] = 0\n",
    "                for word in re.split('[.\\s,?!:;-]', line):\n",
    "                    position['col'] += len(word) + 1\n",
    "                    position['token_no'] += 1\n",
    "                    # Case Folding\n",
    "                    word = word.lower()\n",
    "                    \n",
    "                    # Filter non-ASCII characters\n",
    "                    word = ''.join(filter(lambda x: x in printable, word))\n",
    "                    \n",
    "                    # Remove Punctuations\n",
    "                    word = remove_punctuation(word)\n",
    "                    \n",
    "                    if re.match('\\d+[A-Za-z]+',word):\n",
    "                        word = re.split('\\d+',word)[1]\n",
    "                    if re.match('[A-Za-z]+\\d+',word):\n",
    "                        word = re.split('\\d+',word)[0]\n",
    "                    \n",
    "                    if len(word) == 0 or len(word) == 1 or word == '' or word == ' ':\n",
    "                        continue\n",
    "                    if word in stop_words:\n",
    "                        continue\n",
    "\n",
    "                    word = ps.stem(word)\n",
    "                        \n",
    "                    vocab.add(word)\n",
    "                    \n",
    "                    doc_set.add(word)\n",
    "                    \n",
    "                    if word in inverted_index.index.keys():\n",
    "                        \n",
    "                        inverted_index.index[word].addOccurrance(file_number, copy.deepcopy(position)) \n",
    "                    else:\n",
    "                        plist = PostingList()\n",
    "                        inverted_index.index[word] = plist\n",
    "                        inverted_index.index[word].addOccurrance(file_number, copy.deepcopy(position))\n",
    "                        \n",
    "            inverted_index.docs[file_number] = doc_set\n",
    "            doc_contents.append(doc_set)\n",
    "    ii = InvertedIndexModel()\n",
    "    ii.status = True\n",
    "    ii.data = inverted_index\n",
    "    ii.save()\n",
    "    return True\n",
    "\n",
    "\n",
    "boperators = ['and', 'or']\n",
    "uoperators = ['not']\n",
    "\n",
    "# Token types\n",
    "#\n",
    "# EOF (end-of-file) token is used to indicate that\n",
    "# there is no more input left for lexical analysis\n",
    "LPAREN, RPAREN, EOF, TERM, AND, OR, NOT = (\n",
    "    '(', ')', 'EOF', 'TERM', 'AND','OR', 'NOT'\n",
    ")\n",
    "\n",
    "\n",
    "class Token(object):\n",
    "    def __init__(self, type, value):\n",
    "        self.type = type\n",
    "        self.value = value\n",
    "        self.inverse = False\n",
    "        self.row = []\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\"String representation of the class instance.\n",
    "\n",
    "        Examples:\n",
    "            Token(TERM, Hello)\n",
    "            Token(AND, '&')\n",
    "            Token(NOT, '!')\n",
    "        \"\"\"\n",
    "        return 'Token({type}, {value})'.format(\n",
    "            type=self.type,\n",
    "            value=repr(self.value)\n",
    "        )\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "\n",
    "\n",
    "class Lexer(object):\n",
    "    def __init__(self, text):\n",
    "        # client string input, e.g. \"hello | world & (why | are | you)\"\n",
    "        self.text = text\n",
    "        # self.pos is an index into self.text\n",
    "        self.pos = 0\n",
    "        self.current_char = self.text[self.pos]\n",
    "        \n",
    "\n",
    "    def error(self):\n",
    "        raise Exception('Invalid character')\n",
    "\n",
    "    def advance(self):\n",
    "        \"\"\"Advance the `pos` pointer and set the `current_char` variable.\"\"\"\n",
    "        self.pos += 1\n",
    "        if self.pos > len(self.text) - 1:\n",
    "            self.current_char = None  # Indicates end of input\n",
    "        else:\n",
    "            self.current_char = self.text[self.pos]\n",
    "\n",
    "    def skip_whitespace(self):\n",
    "        while self.current_char is not None and self.current_char.isspace():\n",
    "            self.advance()\n",
    "\n",
    "    def integer(self):\n",
    "\n",
    "        result = ''\n",
    "        while self.current_char is not None and self.current_char.isdigit():\n",
    "            result += self.current_char\n",
    "            self.advance()\n",
    "        return int(result)\n",
    "    def word(self):\n",
    "        result = ''\n",
    "        # while self.current_char is not None and (self.current_char.isalpha() or self.current_char == '_'):\n",
    "        while self.current_char is not None and (self.current_char in printable) and (self.current_char not in (' ', '|','&','!', '(', ')')):\n",
    "            result += self.current_char\n",
    "            self.advance()\n",
    "        return str(result)\n",
    "\n",
    "    def get_next_token(self):\n",
    "        \"\"\"Lexical analyzer (also known as scanner or tokenizer)\n",
    "\n",
    "        This method is responsible for breaking a sentence\n",
    "        apart into tokens. One token at a time.\n",
    "        \"\"\"\n",
    "        while self.current_char is not None:\n",
    "\n",
    "            if self.current_char.isspace():\n",
    "                self.skip_whitespace()\n",
    "                continue\n",
    "                        \n",
    "            if self.current_char == '&':\n",
    "#                 print('gotB' + self.current_char)\n",
    "                self.advance()\n",
    "                return Token(AND, 'AND')\n",
    "            \n",
    "            if self.current_char == '|':\n",
    "#                 print('gotB' + self.current_char)\n",
    "                self.advance()\n",
    "                return Token(OR, 'OR')\n",
    "            \n",
    "            if self.current_char == '!':\n",
    "#                 print('gotB' + self.current_char)\n",
    "                self.advance()\n",
    "                return Token(NOT,'NOT')\n",
    "            \n",
    "\n",
    "            if self.current_char == '(':\n",
    "                self.advance()\n",
    "                return Token(LPAREN, '(')\n",
    "\n",
    "            if self.current_char == ')':\n",
    "                self.advance()\n",
    "                return Token(RPAREN, ')')\n",
    "            \n",
    "            if self.current_char.isalpha():      \n",
    "#                 print('Got token  ' + self.current_char)\n",
    "                return Token(TERM, self.word())\n",
    "            \n",
    "            \n",
    "\n",
    "            self.error()\n",
    "\n",
    "        return Token(EOF, None)\n",
    "\n",
    "class AST(object):\n",
    "    pass\n",
    "\n",
    "\n",
    "class BinOp(AST):\n",
    "    def __init__(self, left, op, right):\n",
    "        self.left = left\n",
    "        self.token = self.op = op\n",
    "        self.right = right\n",
    "        self.inverse = False\n",
    "        self.row = []\n",
    "        self.value = ''\n",
    "        \n",
    "class Num(AST):\n",
    "    def __init__(self, token):\n",
    "        self.token = token\n",
    "        self.value = token.value\n",
    "        self.inverse = False\n",
    "        self.row = []\n",
    "\n",
    "class Parser(object):\n",
    "    def __init__(self, lexer):\n",
    "        self.lexer = lexer\n",
    "        # set current token to the first token taken from the input\n",
    "        self.current_token = self.lexer.get_next_token()\n",
    "\n",
    "    def error(self):\n",
    "        raise Exception('Invalid syntax')\n",
    "\n",
    "    def eat(self, token_type):\n",
    "        # compare the current token type with the passed token\n",
    "        # type and if they match then \"eat\" the current token\n",
    "        # and assign the next token to the self.current_token,\n",
    "        # otherwise raise an exception.\n",
    "        if self.current_token.type == token_type:\n",
    "            self.current_token = self.lexer.get_next_token()\n",
    "        else:\n",
    "            self.error()\n",
    "\n",
    "    def factor(self):\n",
    "        \"\"\"factor : INTEGER | LPAREN expr RPAREN\"\"\"\n",
    "        token = self.current_token\n",
    "        \n",
    "        \n",
    "        if token.type == TERM:\n",
    "            self.eat(TERM)\n",
    "            return Num(token)\n",
    "        \n",
    "        elif token.type == NOT:\n",
    "            self.eat(NOT)\n",
    "            node = self.expr()\n",
    "            node.inverse = True\n",
    "            return node\n",
    "            \n",
    "        \n",
    "        \n",
    "        elif token.type == LPAREN:\n",
    "            self.eat(LPAREN)\n",
    "            node = self.expr()\n",
    "            self.eat(RPAREN)\n",
    "            return node\n",
    "\n",
    "    def term(self):\n",
    "        \n",
    "        node = self.factor()\n",
    "\n",
    "        while self.current_token.type in (AND,):\n",
    "            token = self.current_token\n",
    "         \n",
    "            if token.type == AND:\n",
    "                self.eat(AND)\n",
    "            \n",
    "            node = BinOp(left=node, op=token, right=self.factor())\n",
    "\n",
    "        return node\n",
    "\n",
    "    def expr(self):\n",
    "        node = self.term()\n",
    "\n",
    "        while self.current_token.type in (OR,):\n",
    "            token = self.current_token\n",
    "            if token.type == OR:\n",
    "                self.eat(OR)\n",
    "            \n",
    "            node = BinOp(left=node, op=token, right=self.term())\n",
    "\n",
    "        return node\n",
    "\n",
    "    def parse(self):\n",
    "        return self.expr()\n",
    "\n",
    "\n",
    "class NodeVisitor(object):\n",
    "    def visit(self, node):\n",
    "#         print('Checking Node Name')\n",
    "        \n",
    "        method_name = 'visit_' + type(node).__name__\n",
    "        visitor = getattr(self, method_name, self.generic_visit)\n",
    "        return visitor(node)\n",
    "\n",
    "    def generic_visit(self, node):\n",
    "        raise Exception('No visit_{} method'.format(type(node).__name__))\n",
    "\n",
    "\n",
    "class Interpreter(NodeVisitor):\n",
    "    def __init__(self, parser, index, ps):\n",
    "        self.parser = parser\n",
    "        self.index = index\n",
    "        self.ps = ps\n",
    "\n",
    "    def visit_BinOp(self, node):\n",
    "#         print('Bin OP : ' )\n",
    "#         print(node.token)\n",
    "#         print(node.value)\n",
    "#         print(node.row)\n",
    "#         print(node.inverse)\n",
    "        if node.op.type == AND:\n",
    "#             print('Node => ')\n",
    "#             print(node)\n",
    "            left = self.visit(node.left)   \n",
    "            right = self.visit(node.right)\n",
    "            \n",
    "#             term_index_left = vocab_list.index(ps.stem(left.value))\n",
    "#             term_row_left = term_doc_matrix_np[term_index_left]\n",
    "            \n",
    "#             term_index_right = vocab_list.index(ps.stem(right.value))\n",
    "#             term_row_right = term_doc_matrix_np[term_index_right]\n",
    "            \n",
    "            if left.inverse == True:\n",
    "                left.value = '!' + str(left.value)\n",
    "                term_row_left = self.index.index[self.ps.stem(left.row)]\n",
    "                left.inverse = False\n",
    "            \n",
    "            if right.inverse == True:\n",
    "                right.value = '!' + str(right.value)\n",
    "                term_row_right = self.index.index[self.ps.stem(right.row)]\n",
    "                right.inverse = False\n",
    "            \n",
    "            node.row = intersect_posting(left.row, right.row)\n",
    "            if node.inverse == True:\n",
    "                node.row = inverse_posting(self.index, node.row)\n",
    "                node.inverse = False\n",
    "            \n",
    "            return node\n",
    "        \n",
    "        elif node.op.type == OR:\n",
    "#             print('Node => ')\n",
    "#             print(node)\n",
    "            left = self.visit(node.left)   \n",
    "            right = self.visit(node.right)\n",
    "            \n",
    "#             term_index_left = vocab_list.index(ps.stem(left.value))\n",
    "#             term_row_left = term_doc_matrix_np[term_index_left]\n",
    "            \n",
    "#             term_index_right = vocab_list.index(ps.stem(right.value))\n",
    "#             term_row_right = term_doc_matrix_np[term_index_right]\n",
    "            \n",
    "            if left.inverse == True:\n",
    "                left.value = '!' + str(left.value)\n",
    "                term_row_left = self.index.index([self.ps.stem(left.row)])\n",
    "                left.inverse = False\n",
    "            \n",
    "            if right.inverse == True:\n",
    "                right.value = '!' + str(right.value)\n",
    "                term_row_right = self.index.index([self.ps.stem(right.row)])\n",
    "                right.inverse = False\n",
    "            \n",
    "\n",
    "            node.row = union_posting(left.row, right.row)\n",
    "            if node.inverse == True:\n",
    "                node.row = inverse_posting(self.index, node.row)\n",
    "                node.inverse = False\n",
    "            \n",
    "            return node\n",
    "        \n",
    "\n",
    "    def visit_Num(self, node):\n",
    "#         print('Num  : ' )\n",
    "#         print(node.token)\n",
    "#         print(node.value)\n",
    "#         print(node.row)\n",
    "#         print(node.inverse)\n",
    "        \n",
    "        node.value = node.value.split('_')[0]\n",
    "        if self.ps.stem(node.value) in self.index.index.keys():\n",
    "            \n",
    "            term_docs = self.index.index[self.ps.stem(node.value)]\n",
    "            \n",
    "        else:\n",
    "            term_docs = {}\n",
    "#             print('Term Row')\n",
    "#             print(term_docs)\n",
    "            \n",
    "        node.row = term_docs\n",
    "        if node.inverse == True:\n",
    "            node.row = inverse_posting(self.index, node.row)\n",
    "            node.inverse = False\n",
    "        \n",
    "        return node\n",
    "\n",
    "    def interpret(self):\n",
    "        tree = self.parser.parse()\n",
    "#         print(tree)\n",
    "        return self.visit(tree)\n",
    "\n",
    "def get_boolean_query(query):\n",
    "    text = str(query)\n",
    "    text = text.replace(' and ','&')\n",
    "    text = text.replace(' AND ','&')\n",
    "    text = text.replace(' or ','|')\n",
    "    text = text.replace(' OR ','|')\n",
    "    text = text.replace('NOT', '!')\n",
    "    text = text.replace('not ','!')\n",
    "    \n",
    "    print(text)\n",
    "#     Django Specific Code\n",
    "#     inverted_index_model_obj = InvertedIndexModel.objects.get()\n",
    "#     inverted_index = inverted_index_model_obj.data\n",
    "#     print('Inverted Index')\n",
    "    \n",
    "    ps = PorterStemmer()\n",
    "    lexer = Lexer(text)\n",
    "    parser = Parser(lexer)\n",
    "    interpreter = Interpreter(parser, inverted_index, ps)\n",
    "    result = interpreter.interpret()\n",
    "\n",
    "#     print(result.value)\n",
    "#     print(result.row)\n",
    "    return result.row\n",
    "\n",
    "def positional_intersect(p1, p2, k):\n",
    "    \n",
    "    ip = intersect_posting(p1, p2)\n",
    "    \n",
    "    lip = sorted(list(ip.occurrance))\n",
    "    npl = PostingList()\n",
    "    ans = []\n",
    "    \n",
    "    for doc in lip:\n",
    "#         print(type(p1))\n",
    "        positions1 = p1.occurrance[doc]\n",
    "        positions2 = p2.occurrance[doc]\n",
    "        index_p2 = 0\n",
    "        index_p1 = 0\n",
    "        for pos1 in positions1:\n",
    "            for pos2 in positions2:\n",
    "                if pos2['token_no'] -  pos1['token_no'] == k and pos2['token_no'] -  pos1['token_no'] > 0:\n",
    "                    ans.append({'doc':doc, 'pos1':  pos1, 'pos2':pos2})\n",
    "                    npl.addOccurrance(doc,pos1)\n",
    "                    npl.addOccurrance(doc,pos2)\n",
    "        \n",
    "        \n",
    "    return npl\n",
    "        \n",
    "\n",
    "def get_phrasal_query(query):\n",
    "    text = str(query)\n",
    "\n",
    "    try:\n",
    "        q1, q2 = text.split(' ')\n",
    "    except ValueError as e:\n",
    "        raise ValueError('Invalid Phrasal Query Syntax')\n",
    "    ps = PorterStemmer()\n",
    "    q1 = ps.stem(q1)\n",
    "    q2 = ps.stem(q2)\n",
    "#     Django Specific Code\n",
    "#     inverted_index_model_obj = InvertedIndexModel.objects.get()\n",
    "#     inverted_index = inverted_index_model_obj.data\n",
    "#     print('Inverted Index')\n",
    "    result = [] \n",
    "    p1 = inverted_index.get_term_postings(q1)\n",
    "    p2 = inverted_index.get_term_postings(q2)\n",
    "    \n",
    "    result = positional_intersect(p1, p2, 1)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def get_proximity_query(query):\n",
    "    text = str(query)\n",
    "    try:\n",
    "        q1, q2, q3 = text.split(' ')\n",
    "    except ValueError as e:\n",
    "        raise ValueError('Invalid Proximity Query Syntax')\n",
    "    ps = PorterStemmer()\n",
    "    q1 = ps.stem(q1)\n",
    "    q2 = ps.stem(q2)\n",
    "    k = int(q3[1])+ 1\n",
    "#     Django Specific Code\n",
    "#     inverted_index_model_obj = InvertedIndexModel.objects.get()\n",
    "#     inverted_index = inverted_index_model_obj.data\n",
    "    \n",
    "    print('Inverted Index')\n",
    "    result = [] \n",
    "    \n",
    "    p1 = inverted_index.get_term_postings(q1)\n",
    "    p2 = inverted_index.get_term_postings(q2)\n",
    "    print(p1)\n",
    "    result = positional_intersect(p1, p2, k)\n",
    "    \n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Provided Queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Boolean Queries '"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'running'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!hammer\n",
      "total_cnt : 29 docs : [dict_keys([16, 17, 18, 19, 20, 21, 24, 25, 27, 33, 34, 35, 36, 39, 40, 42, 43, 45, 46, 49, 50, 51, 53, 54])]\n",
      "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 22, 23, 26, 28, 29, 30, 31, 32, 37, 38, 41, 44, 47, 48, 52, 55}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'not hammer'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actions&wanted\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'actions AND wanted'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policies&western\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'policies AND western'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "united|plane\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'united OR plane'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amazing&playing&here\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'amazing AND playing AND here'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "higher&signals&enjoyed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'higher AND signals AND enjoyed'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "higher|signals|enjoyed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'higher OR signals OR enjoyed'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forgiveness|developments|praised\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'forgiveness OR developments OR praised'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pakistan|afghanistan|aid\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'pakistan OR afghanistan OR aid'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "praised|( forgiveness&developments )\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'praised OR ( forgiveness AND developments )'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "officially|(warriors&cartoons)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'officially OR (warriors AND cartoons)'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "businessperson&( nationwide|international )\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'businessperson AND ( nationwide OR international )'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outdated&( personnel|policies)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'outdated AND ( personnel OR policies)'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outdated|( personnel&policies )\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'outdated OR ( personnel AND policies )'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "biggest&( near|box )\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'biggest AND ( near OR box )'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "box&( united|year )\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'box AND ( united OR year )'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "biggest&( plane|wanted|hour)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'biggest AND ( plane OR wanted OR hour)'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "! (united&plane)\n",
      "total_cnt : 34 docs : [dict_keys([0, 1, 2, 32, 33, 34, 35, 36, 17, 19, 52, 24, 25, 26, 27, 29, 30])]\n",
      "{3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 18, 20, 21, 22, 23, 28, 31, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 53, 54, 55}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'NOT (united AND plane)'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "! (higher|signals|enjoyed)\n",
      "total_cnt : 23 docs : [dict_keys([2, 7, 8, 12, 19, 20, 21, 23, 24, 30, 31, 39, 40, 43, 53, 0, 1, 22, 27, 37, 41, 44])]\n",
      "{3, 4, 5, 6, 9, 10, 11, 13, 14, 15, 16, 17, 18, 25, 26, 28, 29, 32, 33, 34, 35, 36, 38, 42, 45, 46, 47, 48, 49, 50, 51, 52, 54, 55}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'NOT (higher OR signals OR enjoyed)'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "' phrasal queries '"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hillary Clinton\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hillary Clinton'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "' positional queries'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after years /1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'after years /1'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "develop solutions /1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'develop solutions /1'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keep out /2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'keep out /2'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fails = []\n",
    "\n",
    "\"\"\" Boolean Queries \"\"\"\n",
    "q = 'running'\n",
    "a = {'0', '1', '10', '11', '12', '16', '17', '18', '19', '2', '20', '21', '22', '24', '25', '26', '27', '28', '3', '30', '32', '33', '34', '35', '36', '37', '39', '4', '40', '41', '44', '45', '46', '47', '5', '50', '51', '52', '53', '6', '8', '9'}\n",
    "r = get_boolean_query(q).occurrance.keys()\n",
    "rs = set([str(x) for x in r])\n",
    "if (len(a.difference(rs)) == 0):\n",
    "    q\n",
    "else:\n",
    "    'Fail ' + q\n",
    "fails.append({q:[a.difference(rs), rs.difference(a)]})\n",
    "\n",
    "q = 'not hammer'\n",
    "a = {'31', '28', '37', '30', '7', '10', '14', '1', '6', '41', '15', '11', '29', '26', '52', '13', '32', '44', '4', '8', '22', '38', '48', '0', '47', '2', '23', '9', '3', '5', '12', '55'}\n",
    "r = get_boolean_query(q)\n",
    "rs = set([str(x) for x in r])\n",
    "if (len(a.difference(rs)) == 0):\n",
    "    q\n",
    "else:\n",
    "    'Fail ' + q\n",
    "fails.append({q:[a.difference(rs), rs.difference(a)]})\n",
    "    \n",
    "q = 'actions AND wanted'\n",
    "a = {'37', '3', '19', '1', '9', '40', '51', '16', '15', '12', '31', '41', '39', '0', '53', '26', '29', '17', '24', '54', '7', '2', '5', '28', '42'}\n",
    "r = get_boolean_query(q).occurrance.keys()\n",
    "rs = set([str(x) for x in r])\n",
    "if (len(a.difference(rs)) == 0):\n",
    "    q\n",
    "else:\n",
    "    'Fail ' + q\n",
    "fails.append({q:[a.difference(rs), rs.difference(a)]})\n",
    "    \n",
    "q = 'policies AND western'\n",
    "a ={'3', '2', '9'}\n",
    "r = get_boolean_query(q).occurrance.keys()\n",
    "rs = set([str(x) for x in r])\n",
    "if (len(a.difference(rs)) == 0):\n",
    "    q\n",
    "else:\n",
    "    'Fail ' + q\n",
    "fails.append({q:[a.difference(rs), rs.difference(a)]})\n",
    "    \n",
    "q = 'united OR plane'\n",
    "a = {'31', '28', '50', '46', '37', '30', '54', '10', '18', '7', '1', '17', '41', '49', '6', '34', '36', '11', '45', '29', '26', '52', '13', '21', '24', '16', '25', '32', '33', '4', '44', '22', '8', '19', '40', '20', '38', '48', '0', '47', '27', '51', '43', '2', '35', '39', '9', '3', '5', '12', '55'}\n",
    "r = get_boolean_query(q).occurrance.keys()\n",
    "rs = set([str(x) for x in r])\n",
    "if (len(a.difference(rs)) == 0):\n",
    "    q\n",
    "else:\n",
    "    'Fail ' + q\n",
    "fails.append({q:[a.difference(rs), rs.difference(a)]})\n",
    "    \n",
    "q = 'amazing AND playing AND here'\n",
    "a = {'51', '8', '0', '13', '39', '18', '19', '41', '20', '40', '16', '21', '50', '11', '27'}\n",
    "r = get_boolean_query(q).occurrance.keys()\n",
    "rs = set([str(x) for x in r])\n",
    "if (len(a.difference(rs)) == 0):\n",
    "    q\n",
    "else:\n",
    "    'Fail ' + q\n",
    "fails.append({q:[a.difference(rs), rs.difference(a)]})\n",
    "    \n",
    "q = 'higher AND signals AND enjoyed'\n",
    "a = {'8'}\n",
    "r = get_boolean_query(q).occurrance.keys()\n",
    "rs = set([str(x) for x in r])\n",
    "if (len(a.difference(rs)) == 0):\n",
    "    q\n",
    "else:\n",
    "    'Fail ' + q\n",
    "fails.append({q:[a.difference(rs), rs.difference(a)]})\n",
    "    \n",
    "q = 'higher OR signals OR enjoyed'\n",
    "a = {'8', '0', '39', '24', '41', '22', '44', '12', '23', '30', '31', '27', '20', '1', '40', '43', '19', '53', '7', '37', '2', '21'}\n",
    "r = get_boolean_query(q).occurrance.keys()\n",
    "rs = set([str(x) for x in r])\n",
    "if (len(a.difference(rs)) == 0):\n",
    "    q\n",
    "else:\n",
    "    'Fail ' + q\n",
    "fails.append({q:[a.difference(rs), rs.difference(a)]})\n",
    "    \n",
    "q = 'forgiveness OR developments OR praised'\n",
    "a = {'3', '8', '18', '20', '32', '43', '9', '17', '36', '44', '52', '42', '37', '38', '16', '31', '5', '2'}\n",
    "r = get_boolean_query(q).occurrance.keys()\n",
    "rs = set([str(x) for x in r])\n",
    "if (len(a.difference(rs)) == 0):\n",
    "    q\n",
    "else:\n",
    "    'Fail ' + q\n",
    "fails.append({q:[a.difference(rs), rs.difference(a)]})\n",
    "    \n",
    "q = 'pakistan OR afghanistan OR aid'\n",
    "a = {'29', '16', '4', '22', '37', '40', '42', '18', '1', '17', '41', '39', '9', '3'}\n",
    "r = get_boolean_query(q).occurrance.keys()\n",
    "rs = set([str(x) for x in r])\n",
    "if (len(a.difference(rs)) == 0):\n",
    "    q\n",
    "else:\n",
    "    'Fail ' + q\n",
    "fails.append({q:[a.difference(rs), rs.difference(a)]})\n",
    "    \n",
    "q = 'praised OR ( forgiveness AND developments )'\n",
    "a = {'36', '42', '44', '2'}\n",
    "r = get_boolean_query(q).occurrance.keys()\n",
    "rs = set([str(x) for x in r])\n",
    "if (len(a.difference(rs)) == 0):\n",
    "    q\n",
    "else:\n",
    "    'Fail ' + q\n",
    "fails.append({q:[a.difference(rs), rs.difference(a)]})\n",
    "    \n",
    "q = 'officially OR (warriors AND cartoons)'\n",
    "a = {'3', '0', '39', '18', '12', '48', '38', '5', '31', '10', '11', '47', '9', '17', '7', '37', '21', '25', '49'}\n",
    "r = get_boolean_query(q).occurrance.keys()\n",
    "rs = set([str(x) for x in r])\n",
    "if (len(a.difference(rs)) == 0):\n",
    "    q\n",
    "else:\n",
    "    'Fail ' + q\n",
    "fails.append({q:[a.difference(rs), rs.difference(a)]})\n",
    "    \n",
    "q = 'businessperson AND ( nationwide OR international )'\n",
    "a = set({})\n",
    "r = get_boolean_query(q).occurrance.keys()\n",
    "rs = set([str(x) for x in r])\n",
    "if (len(a.difference(rs)) == 0):\n",
    "    q\n",
    "else:\n",
    "    'Fail ' + q\n",
    "fails.append({q:[a.difference(rs), rs.difference(a)]})\n",
    "    \n",
    "q = 'outdated AND ( personnel OR policies)'\n",
    "a = {'2', '8', '5'}\n",
    "r = get_boolean_query(q).occurrance.keys()\n",
    "rs = set([str(x) for x in r])\n",
    "if (len(a.difference(rs)) == 0):\n",
    "    q\n",
    "else:\n",
    "    'Fail ' + q\n",
    "fails.append({q:[a.difference(rs), rs.difference(a)]})\n",
    "    \n",
    "q = 'outdated OR ( personnel AND policies )'\n",
    "a = {'42', '8', '18', '5', '2', '11'}\n",
    "r = get_boolean_query(q).occurrance.keys()\n",
    "rs = set([str(x) for x in r])\n",
    "if (len(a.difference(rs)) == 0):\n",
    "    q\n",
    "else:\n",
    "    'Fail ' + q\n",
    "fails.append({q:[a.difference(rs), rs.difference(a)]})\n",
    "    \n",
    "q = 'biggest AND ( near OR box )'\n",
    "a = {'18', '46', '54', '43', '4', '45', '50', '53', '47', '6', '51', '44'}\n",
    "r = get_boolean_query(q).occurrance.keys()\n",
    "rs = set([str(x) for x in r])\n",
    "if (len(a.difference(rs)) == 0):\n",
    "    q\n",
    "else:\n",
    "    'Fail ' + q\n",
    "fails.append({q:[a.difference(rs), rs.difference(a)]})\n",
    "    \n",
    "q = 'box AND ( united OR year )'\n",
    "a = {'18', '46', '4', '45', '50', '9', '47', '23', '54', '44', '25'}\n",
    "r = get_boolean_query(q).occurrance.keys()\n",
    "rs = set([str(x) for x in r])\n",
    "if (len(a.difference(rs)) == 0):\n",
    "    q\n",
    "else:\n",
    "    'Fail ' + q\n",
    "fails.append({q:[a.difference(rs), rs.difference(a)]})\n",
    "    \n",
    "q = 'biggest AND ( plane OR wanted OR hour)'\n",
    "a = {'50', '53', '46', '37', '30', '54', '42', '18', '7', '1', '49', '41', '6', '36', '45', '26', '52', '44', '16', '4', '8', '19', '40', '48', '0', '47', '51', '43', '2', '35', '39'}\n",
    "r = get_boolean_query(q).occurrance.keys()\n",
    "rs = set([str(x) for x in r])\n",
    "if (len(a.difference(rs)) == 0):\n",
    "    q\n",
    "else:\n",
    "    'Fail ' + q\n",
    "fails.append({q:[a.difference(rs), rs.difference(a)]})\n",
    "    \n",
    "q = 'NOT (united AND plane)'\n",
    "a = {'31', '28', '50', '53', '46', '37', '54', '42', '7', '10', '14', '18', '6', '49', '41', '15', '11', '45', '13', '21', '44', '16', '4', '8', '22', '40', '20', '38', '48', '47', '51', '43', '23', '39', '9', '3', '5', '12', '55'}\n",
    "r = get_boolean_query(q)\n",
    "rs = set([str(x) for x in r])\n",
    "if (len(a.difference(rs)) == 0):\n",
    "    q\n",
    "else:\n",
    "    'Fail ' + q\n",
    "fails.append({q:[a.difference(rs), rs.difference(a)]})\n",
    "    \n",
    "q = 'NOT (higher OR signals OR enjoyed)'\n",
    "a = {'3', '52', '16', '36', '10', '14', '34', '9', '17', '42', '55', '50', '49', '4', '18', '13', '29', '48', '28', '5', '38', '26', '35', '45', '11', '54', '51', '6', '47', '32', '46', '15', '33', '25'}\n",
    "r = get_boolean_query(q)\n",
    "rs = set([str(x) for x in r])\n",
    "if (len(a.difference(rs)) == 0):\n",
    "    q\n",
    "else:\n",
    "    'Fail ' + q\n",
    "fails.append({q:[a.difference(rs), rs.difference(a)]})\n",
    "\n",
    "\"\"\" phrasal queries \"\"\"\n",
    "q = 'Hillary Clinton' \n",
    "a = {'1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '14', '16', '17', '18', '19', '20', '21', '22', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54'}\n",
    "r = get_boolean_query(q).occurrance.keys()\n",
    "rs = set([str(x) for x in r])\n",
    "if (len(a.difference(rs)) == 0):\n",
    "    q\n",
    "else:\n",
    "    'Fail ' + q\n",
    "fails.append({q:[a.difference(rs), rs.difference(a)]})\n",
    "\n",
    "\"\"\" positional queries\"\"\"\n",
    "q = 'after years /1'\n",
    "a = {'6', '7', '44'}\n",
    "r = get_boolean_query(q).occurrance.keys()\n",
    "rs = set([str(x) for x in r])\n",
    "if (len(a.difference(rs)) == 0):\n",
    "    q\n",
    "else:\n",
    "    'Fail ' + q\n",
    "fails.append({q:[a.difference(rs), rs.difference(a)]})\n",
    "    \n",
    "q = 'develop solutions /1'\n",
    "a = {'5', '32'}\n",
    "r = get_boolean_query(q).occurrance.keys()\n",
    "rs = set([str(x) for x in r])\n",
    "if (len(a.difference(rs)) == 0):\n",
    "    q\n",
    "else:\n",
    "    'Fail ' + q\n",
    "fails.append({q:[a.difference(rs), rs.difference(a)]})\n",
    "    \n",
    "q = 'keep out /2'\n",
    "a = {'20', '24', '39', '40', '51'} \n",
    "r = get_boolean_query(q).occurrance.keys()\n",
    "rs = set([str(x) for x in r])\n",
    "if (len(a.difference(rs)) == 0):\n",
    "    q\n",
    "else:\n",
    "    'Fail ' + q\n",
    "fails.append({q:[a.difference(rs), rs.difference(a)]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If Result is not Empty, Some Queries are not returning correct docs\n",
    "list(filter(lambda k : 'FAILED : '+str(list(k.keys)[0]) if len(list(k.values())[0][0]) > 0 else None ,fails))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test to check least frequent terms\n",
    "# m = 100\n",
    "# val = []\n",
    "# for p in inverted_index.index:\n",
    "#     print(p)\n",
    "#     if len(p) < m:\n",
    "#         m = len(p)\n",
    "#         val = p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some test queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter Boolean Query: e.g outdated OR ( personnel AND policies ) : outdated OR ( personnel AND policies )\n",
      "outdated|( personnel&policies )\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "total_cnt : 7 docs : [dict_keys([2, 5, 8, 18, 11, 42])]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = input('Enter Boolean Query: e.g outdated OR ( personnel AND policies ) : ')\n",
    "get_boolean_query(q)\n",
    "# Output contains posting list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter phrasal Query: e.g Hillary ClintonHillary Clinton\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "total_cnt : 1116 docs : [dict_keys([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 16, 17, 18, 19, 20, 21, 22, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54])]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = input('Enter phrasal Query: e.g Hillary Clinton')\n",
    "get_phrasal_query(q)\n",
    "# Output contains posting list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter Proximity Query: e.g develop solutions /1 : develop solutions /1\n",
      "Inverted Index\n",
      "total_cnt : 33 docs : [dict_keys([2, 3, 5, 8, 9, 16, 17, 18, 20, 31, 32, 37, 38])]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "total_cnt : 4 docs : [dict_keys([5, 32])]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = input('Enter Proximity Query: e.g develop solutions /1 : ')\n",
    "get_proximity_query(q)\n",
    "# Output contains posting list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set(inverted_index.docs).symmetric_difference(set(inverted_index.get_term_postings('candidaci').occurrance.keys()))\n",
    "# set(inverted_index.get_term_postings('candidaci').occurrance.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set(inverted_index.get_term_postings('candidaci').occurrance.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References :\n",
    "\n",
    "http://www.pyregex.com/\n",
    "http://cs231n.github.io/python-numpy-tutorial/\n",
    "\n",
    "https://www.online-utility.org/text/analyzer.jsp\n",
    "\n",
    "https://stackoverflow.com/questions/2118261/parse-boolean-arithmetic-including-parentheses-with-regex\n",
    "\n",
    "https://regex101.com/r/M8z3U4/1\n",
    "\n",
    "https://iq.opengenus.org/porter-stemmer/\n",
    "\n",
    "https://unnikked.ga/how-to-build-a-boolean-expression-evaluator-518e9e068a65\n",
    "\n",
    "https://ruslanspivak.com/lsbasi-part7/\n",
    "\n",
    "https://github.com/gintas/django-picklefield"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'occupy', 'pursuit', 'sake', 'worry', 'concern', 'matter_to', 'interest_group', 'involvement', 'stake', 'interest', 'pastime', 'interestingness'}\n",
      "{'uninterestingness', 'bore'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "synonyms = []\n",
    "antonyms = []\n",
    "\n",
    "for syn in wordnet.synsets(\"interest\"):\n",
    "    for l in syn.lemmas():\n",
    "        synonyms.append(l.name())\n",
    "        if l.antonyms():\n",
    "             antonyms.append(l.antonyms()[0].name())\n",
    "\n",
    "print(set(synonyms))\n",
    "print(set(antonyms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "613.4px",
    "left": "587.6px",
    "right": "20px",
    "top": "82px",
    "width": "604.8px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
