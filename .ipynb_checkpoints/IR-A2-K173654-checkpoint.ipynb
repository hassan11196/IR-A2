{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and declerations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import string\n",
    "\n",
    "import re\n",
    "import math\n",
    "import pprint\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "import numpy as np\n",
    "import copy\n",
    "from copy import deepcopy\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class VectorSpaceModel(object):\n",
    "    \n",
    "    def tf_natural(self, tf, doc):\n",
    "    #return tf\n",
    "        return tf / sum(doc.values())\n",
    "    def idf_no(self, term):\n",
    "        return 1\n",
    "    def idf_idf(self, term):\n",
    "        return math.log10((len(self.docs.keys()) / (len(self.index[term]))) + 1 )\n",
    "    def norm_cosine(doc):\n",
    "        return 1 / (sum([tf**2 for tf in doc.values()]) + 1)\n",
    "    def norm_no(self, doc):\n",
    "        return 1  \n",
    "    def __init__(self, tf_func = 'natural', idf_func = 'idf', norm_func='none'):\n",
    "        self.tf_functions = {\n",
    "            'natural': self.tf_natural,\n",
    "            # 'logarithm': lambda tf, doc : (0 if tf==0 else 1 + math.log10(tf)),\n",
    "            # 'augmented': lambda tf, doc : (0.5 + ((0.5 * tf)/(self.find_max_tf(doc)))),\n",
    "            # 'boolean' : lambda tf, doc : (1 if tf > 0 else 0),\n",
    "            # 'log_ave': self.tf_log_ave\n",
    "\n",
    "        }\n",
    "        self.idf_functions = {\n",
    "            'no': self.idf_no,\n",
    "            'idf':self.idf_idf,\n",
    "            'prob_idf' : self.prod_idf\n",
    "        }\n",
    "        self.normailization_functions = {\n",
    "            'none' : self.norm_no,\n",
    "            'cosine' : self.norm_cosine,\n",
    "#             'pivoted_unique' : lambda \n",
    "        }\n",
    "        self.tf_func = self.tf_functions[tf_func]\n",
    "        self.idf_func = self.idf_functions[idf_func]\n",
    "        self.norm_func = self.normailization_functions[norm_func]\n",
    "\n",
    "        self.vocab = {}\n",
    "        self.vocab_idf = {}\n",
    "        self.docs = {}\n",
    "        self.docs_char_length = {}\n",
    "        self.occurrance = {}\n",
    "        self.occurrance2 = {}\n",
    "        self.cdocs = {}\n",
    "        self.index = {}\n",
    "        def tf_log_ave(self, tf, doc):\n",
    "    #         print(tf)\n",
    "            return ( (1+math.log10((tf*sum(doc.values()))+1)) / (1 + math.log10(self.find_avg_tf(doc))))\n",
    "    def prod_idf(self, term):\n",
    "#         print('prob idf')\n",
    "#         print(math.log10((len(self.docs.keys()) - (len(self.index[term])) + 1)/(len(self.index[term]))))\n",
    "        if 0 > math.log10((len(self.docs.keys()) - (len(self.index[term])) + 1)/(len(self.index[term]))):\n",
    "            return 0;\n",
    "        else:\n",
    "            return math.log10((len(self.docs.keys()) - (len(self.index[term])) + 1)/(len(self.index[term])))    \n",
    "    \n",
    "    \n",
    "    def find_avg_tf(self, doc):\n",
    "        csum = 0\n",
    "        cnt = 0\n",
    "#         print(f'Doc sum : {sum(doc.values()) }')\n",
    "#         print(sum(doc.values()) / len(doc.keys()))\n",
    "        return (sum(doc.values()) / len(doc.keys()))\n",
    "    \n",
    "    def find_max_tf(self, doc):\n",
    "        max_tf = 0\n",
    "        max_term = None\n",
    "        for term, tf in doc.items():\n",
    "            if tf > max_tf:\n",
    "                max_tf = tf\n",
    "                max_term = term\n",
    "        return max_tf\n",
    "\n",
    "    \n",
    "    \n",
    "    def create_doc(self, docId):\n",
    "        self.docs[docId] = dict.fromkeys(self.vocab, 0)\n",
    "        \n",
    "    def add_term(self, term, docId, position):\n",
    "        if term not in self.vocab.keys():\n",
    "            self.vocab[term] = 1\n",
    "            for Id, docList in self.docs.items():\n",
    "                self.docs[Id][term] = 0\n",
    "        else:\n",
    "            self.vocab[term]+=1\n",
    "        \n",
    "        if term in self.docs[docId].keys():\n",
    "            self.docs[docId][term] += 1\n",
    "        else:\n",
    "            self.docs[docId][term] = 1\n",
    "            \n",
    "        if docId not in self.occurrance.keys():\n",
    "            self.occurrance[docId] = {}\n",
    "            self.occurrance[docId][term] = []\n",
    "            self.occurrance[docId][term].append(position)\n",
    "        else:\n",
    "            if term not in self.occurrance[docId].keys():\n",
    "                self.occurrance[docId][term] = []\n",
    "                self.occurrance[docId][term].append(position)\n",
    "            else:\n",
    "                self.occurrance[docId][term].append(position)\n",
    "                \n",
    "        if term in self.index.keys():\n",
    "            self.index[term].add(docId)\n",
    "        else:\n",
    "            self.index[term] = set()\n",
    "            self.index[term].add(docId)\n",
    "#         if docId not in self.occurrance.keys():\n",
    "#             self.occurrance[docId] = []\n",
    "#         self.occurrance[docId].append(position)\n",
    "    \n",
    "    def get_query_vector(self, query_terms):\n",
    "        query_vector_hash = dict.fromkeys(self.vocab, 0)\n",
    "#         print(query_vector_hash)\n",
    "        query_terms = [lem.lemmatize(word.lower()) for word in query_terms]\n",
    "        print(query_terms)\n",
    "        for term in query_terms:\n",
    "            if term in query_vector_hash.keys():\n",
    "                query_vector_hash[term] += 1\n",
    "\n",
    "        \n",
    "        words_in_query = len(query_terms)\n",
    "        tf = dict.fromkeys(self.vocab_idf, 0) \n",
    "        tf_idf = dict.fromkeys(self.vocab_idf, 0)\n",
    "        \n",
    "        for term,term_cnt in query_vector_hash.items():\n",
    "            if term_cnt <= 0:\n",
    "                continue\n",
    "#                 print(term)\n",
    "#                 print(term_cnt)\n",
    "#                 tf[term] = term_cnt / words_in_query\n",
    "#                 idf[term] = len(self.docs.keys()) / (self.vocab[term])\n",
    "\n",
    "            tf[term] = self.tf_func( term_cnt,query_vector_hash )\n",
    "            tf_idf[term] = tf[term] *  self.vocab_idf[term]\n",
    "            print(f'tf_idf {term}: {tf_idf[term]}')\n",
    "        new_query_vector_hash = {\n",
    "            'tf': tf,\n",
    "            'idf':self.vocab_idf,\n",
    "            'tf_idf':tf_idf\n",
    "        }\n",
    "    \n",
    "    \n",
    "        return new_query_vector_hash\n",
    "    \n",
    "    def dot_product(self, v_hash_1, v_hash_2):\n",
    "#         print(len(v_hash_1))\n",
    "#         print(len(v_hash_2))\n",
    "#         print('vhash')\n",
    "#         print(v_hash_2)\n",
    "        return sum([v_hash_2[x]*y for x,y in v_hash_1.items()])\n",
    "        \n",
    "    def get_magnitude(self, v_hash):\n",
    "        \n",
    "        mag = sum([x**2 for x in v_hash.values()]) ** 0.5 \n",
    "        if mag == 0:\n",
    "            return 1\n",
    "        return mag\n",
    "    \n",
    "    \n",
    "    def get_cosine_sim(self, v_hash_x, v_hash_y):\n",
    "        return (self.dot_product(v_hash_x,v_hash_y)) / (self.get_magnitude(v_hash_x) * self.get_magnitude(v_hash_y)) \n",
    "            \n",
    "    def get_ranking(self, query_vector_hash):\n",
    "        ranked_docs = []\n",
    "        for docId,doc_vector_hash in self.cdocs.items():\n",
    "            cosine_sim = self.get_cosine_sim(doc_vector_hash['tf_idf'], query_vector_hash['tf_idf'])\n",
    "            ranked_docs.append((docId, cosine_sim))\n",
    "        ranked_docs = sorted(ranked_docs, key=lambda x:x[1])\n",
    "        return ranked_docs\n",
    "    \n",
    "    def calculate_tf_idf(self):\n",
    "        cdocs = {}\n",
    "        self.vocab_idf = dict.fromkeys(self.vocab, 0)\n",
    "        for term, term_cnt in self.vocab.items():\n",
    "            self.vocab_idf[term] = self.idf_func(term)\n",
    "            \n",
    "        for docId,doc in self.docs.items():\n",
    "#             print(doc.values())\n",
    "            words_in_d = sum(doc.values())\n",
    "            tf = {}\n",
    "        \n",
    "            tf_idf = {}\n",
    "            for term,term_cnt in doc.items():\n",
    "#                 tf[term] = term_cnt / words_in_d\n",
    "#                 idf[term] = len(self.docs.keys()) / (self.vocab[term])\n",
    "                tf[term] =self.tf_func(term_cnt, doc)\n",
    "                tf_idf[term] = tf[term] * self.vocab_idf[term]\n",
    "            cdocs[docId] = {\n",
    "                'tf': tf,\n",
    "                'tf_idf':tf_idf,\n",
    "                'total_words' : words_in_d\n",
    "            }\n",
    "        self.cdocs = cdocs\n",
    "        return self.cdocs\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_space = VectorSpaceModel()\n",
    "\n",
    "d1= \"Music is a universal language\"\n",
    "d2= \"Music is a miracle\"\n",
    "d3= \"Music is a universal feature of the human experience\"\n",
    "# test_space.create_doc(1)\n",
    "# for word in d1.split(' '):\n",
    "#     test_space.add_term(word, 1)\n",
    "# test_space.create_doc(2)\n",
    "# for word in d2.split(' '):\n",
    "#     test_space.add_term(word, 2)\n",
    "# test_space.create_doc(3)\n",
    "# for word in d3.split(' '):\n",
    "#     test_space.add_term(word, 3)\n",
    "# test_space.calculate_tf_idf()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Punctuation\n",
    "def remove_punctuation(word):\n",
    "    return word.translate(word.maketrans('','',string.punctuation))\n",
    "\n",
    "# Clean Query Term\n",
    "def clean_word(word):\n",
    "    # Case Folding\n",
    "    word = word.lower()\n",
    "     # Filter non-ASCII characters\n",
    "    word = ''.join(filter(lambda x: x in printable, word))\n",
    "#     print(word)\n",
    "    # Remove Punctuations\n",
    "    if word != '(' and word != ')':\n",
    "        word = remove_punctuation(word)\n",
    "#     print(word)\n",
    "    if re.match('\\d+[A-Za-z]+',word):\n",
    "        word = re.split('\\d+',word)[1]\n",
    "    if re.match('[A-Za-z]+\\d+',word):\n",
    "        word = re.split('\\d+',word)[0]\n",
    "#     print(word)\n",
    "    word = ps.stem(word)\n",
    "#     print(word)\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term - Document Indexing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading : "
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'VectorSpaceModel' object has no attribute 'vocab'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-138453279d5a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Loading : '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfile_number\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m56\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m     \u001b[0mvector_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_doc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_number\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'data/Trump Speechs/speech_{file_number}.txt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfile1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfile1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-58478d58e515>\u001b[0m in \u001b[0;36mcreate_doc\u001b[1;34m(self, docId)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcreate_doc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdocId\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdocs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdocId\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfromkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0madd_term\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mterm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdocId\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mposition\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'VectorSpaceModel' object has no attribute 'vocab'"
     ]
    }
   ],
   "source": [
    "vocab = set()\n",
    "doc_contents = []\n",
    "\n",
    "vector_space = VectorSpaceModel(tf_func='natural',idf_func='idf')\n",
    "printable = set(string.printable) \n",
    "raw_data = []\n",
    "# Printable characters are\n",
    "# 0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\n",
    "# !\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~ \\t\\n\\r\\x0b\\x0c\n",
    "\n",
    "\n",
    "lem = WordNetLemmatizer() \n",
    "\n",
    "stop_words = set()\n",
    "with open('Stopword-List.txt', 'r') as stop_word_file:\n",
    "    lines = stop_word_file.readlines()\n",
    "    for line in lines:\n",
    "        stop_words.add(line.split('\\n')[0])\n",
    "    stop_words.remove('')\n",
    "\n",
    "    \n",
    "print('Loading : ', end='')\n",
    "for file_number in range(0, 56):\n",
    "    vector_space.create_doc(file_number)\n",
    "    with open(f'data/Trump Speechs/speech_{file_number}.txt', 'r') as file1:\n",
    "        lines = file1.readlines()\n",
    "#         print(f'File Number : speech_{file_number}.txt' )\n",
    "#         print(lines[0])\n",
    "        position = {'doc':file_number,'row':0, 'col':0, 'token_no':0}\n",
    "        \n",
    "#         {\n",
    "#             'total_count' : 0,\n",
    "#             'postings' : {\n",
    "#                 'count':0,\n",
    "#                 'doc_id':0,\n",
    "#                 'positions':[]\n",
    "#             }\n",
    "#         }\n",
    "        \n",
    "        for line_no,line in enumerate(lines):\n",
    "            # Skip Heading Line\n",
    "            if line_no == 0:\n",
    "                continue\n",
    "            doc_set = set()\n",
    "            # split words at . , whitespace ? ! : ;\n",
    "            position['row'] = line_no \n",
    "            position['col'] = 0\n",
    "            symbols = \"!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~\\n\"\n",
    "            for i in symbols:\n",
    "                line = line.replace(i, ' ')\n",
    "            raw_data.append(line)\n",
    "            for word in re.split('[.\\s,?!:;-]', line):\n",
    "                \n",
    "                position['col'] += len(word) + 1\n",
    "                position['token_no'] += 1\n",
    "                # Case Folding\n",
    "                word = word.lower()\n",
    "                \n",
    "                # Filter non-ASCII characters\n",
    "                word = ''.join(filter(lambda x: x in printable, word))\n",
    "                \n",
    "                # Remove Punctuations\n",
    "                word = remove_punctuation(word)\n",
    "                \n",
    "                if re.match('\\d+[A-Za-z]+',word):\n",
    "                    word = re.split('\\d+',word)[1]\n",
    "                if re.match('[A-Za-z]+\\d+',word):\n",
    "                    word = re.split('\\d+',word)[0]\n",
    "                \n",
    "                if len(word) == 0 or len(word) == 1 or word == '' or word == ' ':\n",
    "                    continue\n",
    "                if word in stop_words:\n",
    "                    continue\n",
    "                \n",
    "                word = lem.lemmatize(word)\n",
    "                    \n",
    "                vocab.add(word)\n",
    "                \n",
    "                doc_set.add(word)\n",
    "                \n",
    "                vector_space.add_term(word, file_number, deepcopy(position))\n",
    "                \n",
    "        doc_contents.append(doc_set)\n",
    "        print('*', end='')\n",
    "doc_term_tf_idf = vector_space.calculate_tf_idf()\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Vocabulary Size \n",
      "6249\n",
      "Total Number of Documents \n",
      "56\n"
     ]
    }
   ],
   "source": [
    "print('Total Vocabulary Size ')\n",
    "print(len(vector_space.index.keys()))\n",
    "print('Total Number of Documents ')\n",
    "print(len(vector_space.docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can't pickle local object 'VectorSpaceModel.__init__.<locals>.<lambda>'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-3cb1caa8c92b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# To save Inverted Index in File\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'pickled/vector_space.p'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mindex_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvector_space\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: Can't pickle local object 'VectorSpaceModel.__init__.<locals>.<lambda>'"
     ]
    }
   ],
   "source": [
    "# To save Inverted Index in File\n",
    "with open('pickled/vector_space.p', 'wb') as index_file:\n",
    "    pickle.dump(vector_space, index_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Hillary clinton\"\n",
    "query_terms = [lem.lemmatize(x) for x in query.split(' ')]\n",
    "print(query_terms)\n",
    "\n",
    "query_vector = vector_space.get_query_vector(query_terms)\n",
    "\n",
    "# cs = vector_space.get_cosine_sim(vector_space.docs[1], query_vector)\n",
    "# print([(x,y) for x,y in query_vector.items() if y > 0])\n",
    "# ranked = vector_space.get_ranking(vector_space.get_query_vector(query_terms))\n",
    "# ranked.reverse()\n",
    "# ranked = set([x for x,y in ranked if y > 0.0005])\n",
    "# print(ranked)\n",
    "\n",
    "vector_space.index['global']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t['massive', 'inflow', 'of', 'refugee']\n",
      "['massive', 'inflow', 'of', 'refugee']\n",
      "tf_idf of: 0.0\n",
      "tf_idf massive: 0.0\n",
      "tf_idf refugee: 0.0\n",
      "tf_idf inflow: 0.1435078169319297\n",
      "[(32, 0.061637227337830246), (49, 0.04033818051700281), (29, 0.03199478155339573), (46, 0.031364145992634956), (50, 0.030425625221019797), (41, 0.03034233545710322), (40, 0.02905172871976029), (39, 0.027636281792523754), (48, 0.02712086482468235), (30, 0.026229874642549562), (54, 0.024160012515265346), (47, 0.024011595226879037)]\n",
      "Normal results\n",
      "results length :  12\n",
      "Result Differnece : \n",
      "{'12', '44', '20', '38', '37', '31', '52'}\n",
      "Symmetric Difference : \n",
      "{'12', '44', '20', '38', '37', '31', '52'}\n",
      "\n",
      "precision : 1.0\n",
      "recall : 0.7307692307692307\n",
      "\t['pakistan', 'afghanistan']\n",
      "['pakistan', 'afghanistan']\n",
      "tf_idf afghanistan: 0.2870156338638594\n",
      "tf_idf pakistan: 0.8740940135031002\n",
      "[(3, 0.06127678515105877), (41, 0.009465902613877383), (40, 0.00906327185047508), (39, 0.008621694675663497), (16, 0.008192686440977872), (22, 0.007713382987150631), (18, 0.006064794140602565), (4, 0.006011023847093988), (37, 0.005470071008676793), (17, 0.005212383522038264), (1, 0.0034938488966243034), (9, 0.0026719246283192196)]\n",
      "Normal results\n",
      "results length :  12\n",
      "Result Differnece : \n",
      "set()\n",
      "Symmetric Difference : \n",
      "{'9', '40', '18', '37', '39', '41'}\n",
      "\n",
      "precision : 0.5\n",
      "recall : 1.0\n",
      "\t['hillary', 'clinton']\n",
      "['hillary', 'clinton']\n",
      "tf_idf clinton: 0.0\n",
      "tf_idf hillary: 0.0\n",
      "[]\n",
      "Normal results\n",
      "results length :  0\n",
      "Result Differnece : \n",
      "{'33', '17', '16', '20', '28', '47', '54', '51', '35', '10', '41', '49', '30', '12', '44', '9', '50', '18', '24', '26', '19', '27', '14', '36', '25', '21', '43', '45', '5', '3', '40', '8', '48', '29', '37', '1', '55', '31', '52', '46', '22', '6', '53', '32', '38', '42', '2', '34', '7', '39', '11', '4'}\n",
      "Symmetric Difference : \n",
      "{'33', '17', '16', '20', '28', '47', '54', '51', '35', '10', '41', '30', '49', '12', '44', '9', '50', '18', '24', '26', '19', '27', '14', '36', '25', '21', '43', '45', '5', '3', '40', '8', '48', '29', '37', '1', '55', '31', '52', '46', '22', '6', '53', '32', '38', '42', '2', '34', '7', '39', '11', '4'}\n",
      "\n",
      "precision : 1.0\n",
      "recall : 0.5\n",
      "\t['personnel', 'policy']\n",
      "['personnel', 'policy']\n",
      "tf_idf policy: 0.0\n",
      "tf_idf personnel: 0.5611079391364133\n",
      "[(5, 0.08541512214914662), (42, 0.06191672151001794), (11, 0.04167297177282565), (18, 0.03800527658332206)]\n",
      "Normal results\n",
      "results length :  4\n",
      "Result Differnece : \n",
      "{'22', '17', '27', '25', '10', '29'}\n",
      "Symmetric Difference : \n",
      "{'22', '17', '42', '27', '25', '10', '29'}\n",
      "\n",
      "precision : 0.9\n",
      "recall : 0.6\n",
      "\t['united', 'plane']\n",
      "['united', 'plane']\n",
      "tf_idf united: 0.0\n",
      "tf_idf plane: 0.18580553497484423\n",
      "[(35, 0.027928590914416643), (27, 0.023802313219982663), (34, 0.02371186899580704), (33, 0.023177926228891244), (25, 0.022970341423029977), (29, 0.020712486713360644), (32, 0.01995106999172028), (24, 0.01969773652219378), (30, 0.016980454425676864), (36, 0.016469830239972536), (52, 0.015384951785023243), (26, 0.01240497698115923), (17, 0.01081624286088757), (2, 0.009960514051627152), (1, 0.007250103148656048), (19, 0.005239987993952783), (0, 0.0049003319667015625)]\n",
      "Normal results\n",
      "results length :  17\n",
      "Result Differnece : \n",
      "{'16', '20', '28', '47', '54', '51', '10', '41', '49', '12', '44', '9', '13', '50', '18', '21', '43', '45', '5', '3', '40', '8', '48', '55', '37', '31', '46', '22', '38', '7', '39', '11', '4'}\n",
      "Symmetric Difference : \n",
      "{'16', '20', '28', '47', '54', '51', '10', '41', '0', '49', '12', '44', '9', '13', '50', '18', '21', '43', '45', '5', '3', '40', '8', '48', '37', '55', '31', '46', '22', '38', '7', '39', '11', '4'}\n",
      "\n",
      "precision : 0.98\n",
      "recall : 0.5975609756097561\n",
      "\t['develop', 'solution']\n",
      "['develop', 'solution']\n",
      "tf_idf solution: 0.18580553497484423\n",
      "tf_idf develop: 0.33604892896785876\n",
      "[(38, 0.06421267300017991), (2, 0.06306115788819586), (23, 0.06171183811451456), (32, 0.05088574438354171), (18, 0.039838806890960185), (17, 0.03423943760694141), (21, 0.030878416164771577), (16, 0.026908320832789), (20, 0.024122286197931504), (3, 0.023307420588393295), (9, 0.017551509058184464), (39, 0.017313910227218683), (30, 0.01643280735960503), (35, 0.013513924386730002), (27, 0.011517325097775286), (33, 0.011215200346421947), (25, 0.011114755416033599), (24, 0.009531226361056707), (40, 0.009100338221611147), (51, 0.008185856751327545), (52, 0.007444381127333135), (31, 0.005842607164700369), (5, 0.004562030715136633), (8, 0.004013235512550257), (1, 0.0035081378092854444)]\n",
      "Normal results\n",
      "results length :  25\n",
      "Result Differnece : \n",
      "set()\n",
      "Symmetric Difference : \n",
      "{'8', '1', '31', '25'}\n",
      "\n",
      "precision : 0.84\n",
      "recall : 1.0\n",
      "\t['development', 'praised']\n",
      "['development', 'praised']\n",
      "tf_idf development: 0.7196663469151313\n",
      "tf_idf praised: 0.7196663469151313\n",
      "[(31, 0.06613963173614636), (36, 0.04510728341565205), (44, 0.029306483986071642), (8, 0.006944356363608301)]\n",
      "Normal results\n",
      "results length :  4\n",
      "Result Differnece : \n",
      "set()\n",
      "Symmetric Difference : \n",
      "{'31'}\n",
      "\n",
      "precision : 0.75\n",
      "recall : 1.0\n",
      "\t['muslim']\n",
      "['muslim']\n",
      "tf_idf muslim: 0.853871964321762\n",
      "[(3, 0.12956890515368186), (4, 0.0859832554553467), (9, 0.05095987721188214), (2, 0.04577368322245547), (20, 0.03501889035983625), (7, 0.02379017844475003), (6, 0.014020098584988101)]\n",
      "Normal results\n",
      "results length :  7\n",
      "Result Differnece : \n",
      "set()\n",
      "Symmetric Difference : \n",
      "{'7', '6', '9', '20', '2'}\n",
      "\n",
      "precision : 0.2857142857142857\n",
      "recall : 1.0\n",
      "\t['american', 'energy', 'revolution']\n",
      "['american', 'energy', 'revolution']\n",
      "tf_idf american: 0.0\n",
      "tf_idf energy: 0.0\n",
      "tf_idf revolution: 0.284623988107254\n",
      "[(30, 0.0780339185132018), (31, 0.055489183487371696), (13, 0.049410839864838456), (52, 0.03535088177930748), (12, 0.03175859732344077), (18, 0.028917466593627737), (9, 0.012739969302970533)]\n",
      "Normal results\n",
      "results length :  7\n",
      "Result Differnece : \n",
      "{'15', '33', '17', '16', '20', '28', '47', '54', '51', '35', '10', '41', '49', '44', '50', '24', '26', '27', '19', '14', '36', '25', '21', '43', '45', '5', '3', '40', '23', '8', '48', '29', '37', '55', '1', '46', '22', '32', '53', '42', '2', '34', '7', '39', '11', '4'}\n",
      "Symmetric Difference : \n",
      "{'33', '16', '28', '47', '35', '10', '24', '14', '36', '21', '43', '45', '23', '8', '29', '37', '1', '22', '32', '34', '11', '4', '15', '17', '20', '54', '51', '41', '49', '44', '50', '26', '27', '19', '25', '5', '3', '40', '48', '55', '46', '53', '42', '2', '7', '39'}\n",
      "\n",
      "precision : 1.0\n",
      "recall : 0.5353535353535354\n",
      "\t['future', 'of', 'new', 'america']\n",
      "['future', 'of', 'new', 'america']\n",
      "tf_idf of: 0.0\n",
      "tf_idf new: 0.0\n",
      "tf_idf america: 0.0\n",
      "tf_idf future: 0.0\n",
      "[]\n",
      "Normal results\n",
      "results length :  0\n",
      "Result Differnece : \n",
      "{'15', '33', '17', '16', '20', '47', '54', '51', '35', '10', '41', '30', '49', '12', '44', '9', '13', '50', '18', '24', '26', '27', '19', '14', '25', '36', '21', '43', '45', '5', '3', '40', '23', '8', '48', '29', '37', '1', '55', '31', '52', '46', '22', '6', '32', '38', '42', '2', '34', '7', '39', '11', '4'}\n",
      "Symmetric Difference : \n",
      "{'15', '33', '17', '16', '20', '47', '54', '51', '35', '10', '41', '49', '30', '12', '44', '9', '13', '50', '18', '24', '26', '27', '19', '14', '25', '36', '21', '43', '45', '5', '3', '40', '23', '8', '48', '29', '37', '1', '55', '31', '52', '46', '22', '6', '32', '38', '42', '2', '34', '7', '39', '11', '4'}\n",
      "\n",
      "precision : 1.0\n",
      "recall : 0.5\n",
      "\t['hillary', 'clinton', 'is', 'the', 'worst', 'looser']\n",
      "['hillary', 'clinton', 'is', 'the', 'worst', 'looser']\n",
      "tf_idf is: 0.0\n",
      "tf_idf worst: 0.034575674541570366\n",
      "tf_idf clinton: 0.0\n",
      "tf_idf hillary: 0.0\n",
      "[(49, 0.009718772335701849), (25, 0.008548884717120656), (3, 0.008394574329625175), (9, 0.008254034341347534), (7, 0.00770664925479714), (46, 0.007556637173499943), (32, 0.007425200792680419), (2, 0.007414020086375257), (24, 0.0073309175347137405), (41, 0.007310449967290467), (40, 0.006999501062446531), (39, 0.006658474118177637), (48, 0.006534293500603749), (54, 0.005820928417051412), (47, 0.005785169892050495), (45, 0.005043658873668472), (10, 0.004727885628479949), (31, 0.004493825840413405), (17, 0.004025487109103232), (0, 0.0036475185359632198), (1, 0.002698274913012518), (6, 0.0022708527084634965), (19, 0.0019501692401701232), (8, 0.0018873239691636974)]\n",
      "Normal results\n",
      "results length :  24\n",
      "Result Differnece : \n",
      "{'33', '16', '20', '28', '51', '35', '30', '12', '44', '50', '18', '26', '27', '14', '36', '21', '43', '5', '29', '37', '55', '52', '22', '53', '38', '42', '34', '11', '4'}\n",
      "Symmetric Difference : \n",
      "{'33', '16', '20', '28', '51', '35', '0', '30', '12', '44', '50', '18', '26', '27', '14', '36', '21', '43', '5', '29', '37', '55', '52', '22', '53', '38', '42', '34', '11', '4'}\n",
      "\n",
      "precision : 0.9811320754716981\n",
      "recall : 0.6419753086419753\n",
      "\t['no', 'patience', 'for', 'injustice']\n",
      "['no', 'patience', 'for', 'injustice']\n",
      "tf_idf patience: 0.7196663469151313\n",
      "tf_idf injustice: 0.5085166696493901\n",
      "[(11, 0.06544572216980714), (7, 0.04910301361039902), (16, 0.02685003989186472), (22, 0.025279209987866362), (15, 0.02059429367212885)]\n",
      "Normal results\n",
      "results length :  5\n",
      "Result Differnece : \n",
      "set()\n",
      "Symmetric Difference : \n",
      "set()\n",
      "\n",
      "precision : 1.0\n",
      "recall : 1.0\n",
      "\t['global', 'interest']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['global', 'interest']\n",
      "tf_idf interest: 0.0\n",
      "tf_idf global: 0.24367020995067426\n",
      "[(44, 0.09823087175559711), (42, 0.08066501724975009), (36, 0.06479689089406422), (46, 0.026627497380766516), (32, 0.026164351962288217), (40, 0.024664303965866313), (16, 0.02229513932837816), (52, 0.02017622581612094), (20, 0.01998674443655923), (43, 0.01770371105286169), (31, 0.015834998167552253), (9, 0.014542465964605308), (2, 0.013062477124296788), (1, 0.009507973789027567)]\n",
      "Normal results\n",
      "results length :  14\n",
      "Result Differnece : \n",
      "{'22', '18', '24', '27', '47', '54', '8', '25', '48', '10', '21', '41', '39', '11'}\n",
      "Symmetric Difference : \n",
      "{'20', '47', '54', '10', '41', '44', '9', '18', '24', '27', '36', '25', '21', '43', '8', '48', '1', '31', '52', '46', '22', '32', '2', '39', '11'}\n",
      "\n",
      "precision : 0.6071428571428571\n",
      "recall : 0.5483870967741935\n",
      "\t['pakistan', 'afghanistan', 'aid']\n",
      "['pakistan', 'afghanistan', 'aid']\n",
      "tf_idf afghanistan: 0.1913437559092396\n",
      "tf_idf pakistan: 0.5827293423354001\n",
      "tf_idf aid: 0.3098063085714309\n",
      "[(3, 0.05469529201040778), (41, 0.030598912239689555), (40, 0.029297392046962807), (39, 0.027869976007492796), (22, 0.02493382181525092), (29, 0.02335597888897691), (42, 0.023119903500974984), (16, 0.0073127429275922215), (18, 0.005413398984387414), (4, 0.005365403941930685), (37, 0.004882552673083135), (17, 0.00465254236339771), (1, 0.0031185886330364287), (9, 0.00238494394598955)]\n",
      "Normal results\n",
      "results length :  14\n",
      "Result Differnece : \n",
      "set()\n",
      "Symmetric Difference : \n",
      "{'9', '18', '37'}\n",
      "\n",
      "precision : 0.7857142857142857\n",
      "recall : 1.0\n",
      "\t['biggest', 'plane', 'wanted', 'hour']\n",
      "['biggest', 'plane', 'wanted', 'hour']\n",
      "tf_idf wanted: 0.1435078169319297\n",
      "tf_idf plane: 0.09290276748742211\n",
      "tf_idf biggest: 0.0\n",
      "tf_idf hour: 0.10216596851595268\n",
      "[(0, 0.04868626000043731), (35, 0.0441150385092076), (8, 0.03529796642415267), (52, 0.032980856213857185), (34, 0.024438034506669624), (11, 0.023145079753250686), (24, 0.02030097099970396), (51, 0.018830406999298124), (2, 0.015733284300147563), (19, 0.015627334266261724), (9, 0.015470950424387545), (43, 0.015026178676346098), (3, 0.01255320726549015), (27, 0.01110333336669391), (41, 0.011081320662988463), (33, 0.010812068528363208), (25, 0.010715234103908905), (5, 0.01063763694516959), (40, 0.010609978332516588), (28, 0.010532084229236459), (39, 0.010093043131390608), (29, 0.009661987165120962), (32, 0.009306800523670897), (30, 0.007921063993392007), (36, 0.007682867373316778), (15, 0.007356272183114091), (10, 0.007166619967517438), (6, 0.00679164507916503), (26, 0.005786689451357262), (17, 0.00504557473677532), (55, 0.00470142515018814), (1, 0.0033820373447931594)]\n",
      "Normal results\n",
      "results length :  32\n",
      "Result Differnece : \n",
      "set()\n",
      "Symmetric Difference : \n",
      "{'15', '33', '17', '28', '51', '35', '10', '41', '0', '30', '9', '24', '26', '27', '25', '36', '43', '5', '3', '40', '8', '29', '55', '6', '32', '2', '34', '39', '11'}\n",
      "\n",
      "precision : 0.09375\n",
      "recall : 1.0\n",
      "\t['near', 'architect', 'box']\n",
      "['near', 'architect', 'box']\n",
      "tf_idf box: 0.1913437559092396\n",
      "tf_idf near: 0.2623686976788567\n",
      "tf_idf architect: 0.41842416836776863\n",
      "[(54, 0.06687617025266954), (24, 0.052564667898715646), (39, 0.04774306614101994), (18, 0.020227668647371626), (51, 0.017750058852254714), (53, 0.014813970841889717), (11, 0.014478875114037986), (43, 0.014164088744315486), (25, 0.012818585873684885), (17, 0.011348659005018275), (46, 0.011330764857642579), (50, 0.010991710251160332), (9, 0.008911564808203525), (47, 0.008674546389260773), (45, 0.0075626911028772986), (23, 0.0071171921170006406), (4, 0.006960832518337772), (6, 0.006401991197710109), (44, 0.005971436721477655)]\n",
      "Normal results\n",
      "results length :  19\n",
      "Result Differnece : \n",
      "set()\n",
      "Symmetric Difference : \n",
      "{'47', '46', '6', '44', '50', '53', '25', '45'}\n",
      "\n",
      "precision : 0.5789473684210527\n",
      "recall : 1.0\n",
      "\t['peaceful', 'change']\n",
      "['peaceful', 'change']\n",
      "tf_idf change: 0.0\n",
      "tf_idf peaceful: 0.33604892896785876\n",
      "[(10, 0.04595140724797723), (4, 0.045119302990412266), (14, 0.042104720043378806), (7, 0.037451347838373956), (2, 0.03602928276029866), (48, 0.031754150302974814), (30, 0.03071094477303276), (11, 0.024958045599424786), (3, 0.020397199404019902), (17, 0.019562317286993747)]\n",
      "Normal results\n",
      "results length :  10\n",
      "Result Differnece : \n",
      "{'33', '16', '20', '47', '54', '51', '35', '41', '49', '12', '50', '13', '25', '36', '21', '43', '45', '5', '23', '8', '29', '37', '31', '52', '46', '22', '53', '32', '42', '39'}\n",
      "Symmetric Difference : \n",
      "{'33', '16', '20', '47', '54', '51', '35', '41', '49', '12', '50', '13', '25', '36', '21', '43', '45', '5', '23', '8', '29', '37', '31', '52', '46', '22', '53', '32', '42', '39'}\n",
      "\n",
      "precision : 1.0\n",
      "recall : 0.5714285714285714\n",
      "normal f1 : 0.04589559259304809\n"
     ]
    }
   ],
   "source": [
    "test = [(None, None) for x in range(0, 17)] \n",
    "test[0] =(\"massive inflow of refugees\",\n",
    "{'32', '50', '49', '47', '46', '29', '48', '54', '41', '40', '30', '39', '12', '52', '37', '44', '31', '38', '20'}\n",
    "         )\n",
    "test[1] =('pakistan afghanistan',\n",
    "{'3', '22', '16', '17', '4', '1'}\n",
    "         )\n",
    "test[2] =('Hillary Clinton',\n",
    "{'4', '12', '36', '42', '33', '20', '29', '35', '16', '22', '47', '34', '14', '21', '45', '46', '41', '11', '3', '51', '53', '40', '37', '17', '48', '39', '5', '28', '10', '18', '54', '44', '25', '43', '49', '24', '30', '32', '8', '50', '9', '26', '7', '19', '52', '31', '27', '6', '2', '38', '1', '55'}\n",
    "         )\n",
    "test[3] =('personnel policies',\n",
    "\n",
    "{'5', '18', '11', '25', '10', '22', '29', '27', '17'}\n",
    "         )\n",
    "\n",
    "test[4] =('united plane',\n",
    "{'38', '1', '25', '41', '48', '29', '3', '17', '49', '20', '4', '52', '13', '39', '46', '22', '40', '36', '28', '12', '45', '26', '2', '51', '50', '19', '24', '5', '47', '31', '35', '21', '37', '55', '9', '33', '44', '54', '34', '7', '32', '43', '30', '16', '27', '18', '11', '10', '8'}\n",
    "         )\n",
    "\n",
    "test[5] =('develop solutions',\n",
    "{'38', '23', '2', '17', '32', '18', '51', '16', '39', '35', '21', '27', '33', '20', '24', '5', '3', '9', '40', '52', '30'}\n",
    "         )\n",
    "\n",
    "test[6] =('developments praised',\n",
    "{'36', '44', '8'}\n",
    "         )\n",
    "test[7] =(\"muslims\",\n",
    "{'4', '3'}\n",
    "         )\n",
    "test[8] =('American Energy Revolution',\n",
    "\n",
    "{'13', '28', '35', '33', '51', '27', '12', '36', '34', '14', '21', '29', '31', '30', '25', '32', '26', '24', '11', '54', '49', '16', '46', '48', '10', '18', '4', '50', '42', '5', '23', '47', '20', '43', '37', '39', '41', '45', '2', '22', '3', '40', '7', '17', '52', '44', '53', '19', '9', '55', '1', '15', '8'}\n",
    "         )\n",
    "test[9] =('Future of new America',\n",
    "{'51', '14', '35', '26', '13', '50', '33', '4', '12', '31', '20', '32', '29', '34', '41', '27', '46', '30', '40', '25', '17', '42', '16', '24', '54', '2', '43', '21', '7', '18', '52', '49', '5', '47', '45', '10', '36', '38', '44', '11', '3', '39', '22', '9', '48', '37', '23', '15', '1', '19', '55', '6', '8'}\n",
    "         )\n",
    "test[10] =('Hillary clinton is the worst looser',\n",
    "{'4', '12', '36', '42', '33', '20', '29', '35', '16', '22', '47', '34', '14', '21', '45', '46', '41', '11', '3', '51', '53', '40', '37', '17', '48', '39', '5', '28', '10', '18', '54', '44', '25', '43', '49', '24', '30', '32', '8', '50', '9', '26', '7', '19', '52', '31', '27', '6', '2', '38', '1', '55'}\n",
    "          )\n",
    "test[11] =('no patience for injustice',\n",
    "{'11', '22', '16', '7', '15'}\n",
    "          )\n",
    "test[12] =('Global interests',\n",
    "{'10', '16', '27', '42', '21', '11', '25', '22', '47', '41', '48', '54', '18', '40', '24', '39', '8'}\n",
    "          )\n",
    "test[13] =('pakistan afghanistan aid',\n",
    "{'3', '22', '42', '29', '41', '16', '40', '39', '17', '4', '1'}\n",
    "          )\n",
    "test[14] =('biggest plane wanted hour',\n",
    "{'52', '1', '19'}\n",
    "          )\n",
    "test[15] =('near architect box',\n",
    "{'54', '23', '18', '24', '39', '43', '51', '4', '9', '17', '11'}\n",
    "          )\n",
    "test[16] =('peaceful change',\n",
    "{'14', '10', '4', '30', '3', '11', '20', '16', '50', '52', '48', '17', '7', '2', '22', '29', '13', '53', '41', '46', '32', '21', '39', '25', '33', '54', '42', '5', '36', '31', '43', '51', '49', '23', '47', '35', '37', '12', '45', '8'}\n",
    "         )\n",
    "cumulative_precision_normal = 0\n",
    "cumulative_recall_normal = 0 \n",
    "cumulative_precision_sciket = 0\n",
    "cumulative_recall_sciket = 0 \n",
    "cumulative_precision_group = 0\n",
    "cumulative_recall_group = 0 \n",
    "\n",
    "cnt = 0\n",
    "for t in test:\n",
    "    cnt+=1\n",
    "    normal_cnt = [0, 0 , 0]\n",
    "    # my custom model\n",
    "    query = t[0]\n",
    "    query_terms = [lem.lemmatize(x.lower()) for x in query.split(' ')]\n",
    "    print('\\t' + str(query_terms))\n",
    "    # cs = vector_space.get_cosine_sim(vector_space.docs[1], query_vector)\n",
    "    # print([(x,y) for x,y in query_vector.items() if y > 0])\n",
    "    ranked = vector_space.get_ranking(vector_space.get_query_vector(query_terms))\n",
    "    ranked.reverse()\n",
    "    print(str([(x,y) for x,y in ranked if y > 0.0000]))\n",
    "    ranked = set([str(x) for x,y in ranked if y > 0.0000])\n",
    "    \n",
    "    print('Normal results')\n",
    "    print(f'results length :  {len(ranked)}')\n",
    "    print('Result Differnece : ')\n",
    "    print(t[1].difference(ranked))\n",
    "    print('Symmetric Difference : ')\n",
    "    print(t[1].symmetric_difference(ranked))\n",
    "    print()\n",
    "    normal_cnt = [len(t[1]), len(ranked.difference(t[1])), len(t[1].difference(ranked))]\n",
    "    normal_p = normal_cnt[0] / (normal_cnt[0] + normal_cnt[1])\n",
    "    normal_r = normal_cnt[0] / (normal_cnt[0] + normal_cnt[2])\n",
    "    print(f'precision : {normal_p}')\n",
    "    print(f'recall : {normal_r}')\n",
    "    cumulative_precision_normal += normal_p\n",
    "    cumulative_recall_normal += normal_r\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "f1_normal = (2 * (cumulative_precision_normal / cnt) * (cumulative_recall_normal / cnt) ) / (cumulative_recall_normal+cumulative_precision_normal)\n",
    "\n",
    "\n",
    "print(f'normal f1 : {f1_normal}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter Your Query  : muslims\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'WordNetLemmatizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-d43155225de0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mquery\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Enter Your Query  : '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mlem\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mquery_terms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mlem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mquery\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\t'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery_terms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# cs = vector_space.get_cosine_sim(vector_space.docs[1], query_vector)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'WordNetLemmatizer' is not defined"
     ]
    }
   ],
   "source": [
    "query = input('Enter Your Query  : ')\n",
    "lem = WordNetLemmatizer()\n",
    "query_terms = [lem.lemmatize(x.lower()) for x in query.split(' ')]\n",
    "print('\\t' + str(query_terms))\n",
    "# cs = vector_space.get_cosine_sim(vector_space.docs[1], query_vector)\n",
    "# print([(x,y) for x,y in query_vector.items() if y > 0])\n",
    "ranked = vector_space.get_ranking(vector_space.get_query_vector(query_terms))\n",
    "ranked.reverse()\n",
    "print(str([(x,y) for x,y in ranked if y > 0.0000]))\n",
    "ranked = set([str(x) for x,y in ranked if y > 0.0000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References :\n",
    "\n",
    "http://www.pyregex.com/\n",
    "http://cs231n.github.io/python-numpy-tutorial/\n",
    "\n",
    "https://www.online-utility.org/text/analyzer.jsp\n",
    "\n",
    "https://stackoverflow.com/questions/2118261/parse-boolean-arithmetic-including-parentheses-with-regex\n",
    "\n",
    "https://regex101.com/r/M8z3U4/1\n",
    "\n",
    "https://iq.opengenus.org/porter-stemmer/\n",
    "\n",
    "https://unnikked.ga/how-to-build-a-boolean-expression-evaluator-518e9e068a65\n",
    "\n",
    "https://ruslanspivak.com/lsbasi-part7/\n",
    "\n",
    "https://github.com/gintas/django-picklefield"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "613.4px",
    "left": "645.6px",
    "right": "20px",
    "top": "84px",
    "width": "604.8px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
