{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and declerations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import string\n",
    "\n",
    "import re\n",
    "import math\n",
    "import pprint\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "import numpy as np\n",
    "import copy\n",
    "from copy import deepcopy\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class VectorSpaceModel(object):\n",
    "    \n",
    "    def tf_natural(self, tf, doc):\n",
    "    #return tf\n",
    "        return tf / sum(doc.values())\n",
    "    def idf_no(self, term):\n",
    "        return 1\n",
    "    def idf_idf(self, term):\n",
    "        return math.log10((len(self.docs.keys()) / (len(self.index[term]))) + 1 )\n",
    "    def norm_cosine(doc):\n",
    "        return 1 / (sum([tf**2 for tf in doc.values()]) + 1)\n",
    "    def norm_no(self, doc):\n",
    "        return 1  \n",
    "    def __init__(self, tf_func = 'natural', idf_func = 'idf', norm_func='none'):\n",
    "        self.tf_functions = {\n",
    "            'natural': self.tf_natural,\n",
    "            # 'logarithm': lambda tf, doc : (0 if tf==0 else 1 + math.log10(tf)),\n",
    "            # 'augmented': lambda tf, doc : (0.5 + ((0.5 * tf)/(self.find_max_tf(doc)))),\n",
    "            # 'boolean' : lambda tf, doc : (1 if tf > 0 else 0),\n",
    "            # 'log_ave': self.tf_log_ave\n",
    "\n",
    "        }\n",
    "        self.idf_functions = {\n",
    "            'no': self.idf_no,\n",
    "            'idf':self.idf_idf,\n",
    "            'prob_idf' : self.prod_idf\n",
    "        }\n",
    "        self.normailization_functions = {\n",
    "            'none' : self.norm_no,\n",
    "            'cosine' : self.norm_cosine,\n",
    "#             'pivoted_unique' : lambda \n",
    "        }\n",
    "        self.tf_func = self.tf_functions[tf_func]\n",
    "        self.idf_func = self.idf_functions[idf_func]\n",
    "        self.norm_func = self.normailization_functions[norm_func]\n",
    "\n",
    "        self.vocab = {}\n",
    "        self.vocab_idf = {}\n",
    "        self.docs = {}\n",
    "        self.docs_char_length = {}\n",
    "        self.occurrance = {}\n",
    "        self.occurrance2 = {}\n",
    "        self.cdocs = {}\n",
    "        self.index = {}\n",
    "        def tf_log_ave(self, tf, doc):\n",
    "    #         print(tf)\n",
    "            return ( (1+math.log10((tf*sum(doc.values()))+1)) / (1 + math.log10(self.find_avg_tf(doc))))\n",
    "    def prod_idf(self, term):\n",
    "#         print('prob idf')\n",
    "#         print(math.log10((len(self.docs.keys()) - (len(self.index[term])) + 1)/(len(self.index[term]))))\n",
    "        if 0 > math.log10((len(self.docs.keys()) - (len(self.index[term])) + 1)/(len(self.index[term]))):\n",
    "            return 0;\n",
    "        else:\n",
    "            return math.log10((len(self.docs.keys()) - (len(self.index[term])) + 1)/(len(self.index[term])))    \n",
    "    \n",
    "    \n",
    "    def find_avg_tf(self, doc):\n",
    "        csum = 0\n",
    "        cnt = 0\n",
    "#         print(f'Doc sum : {sum(doc.values()) }')\n",
    "#         print(sum(doc.values()) / len(doc.keys()))\n",
    "        return (sum(doc.values()) / len(doc.keys()))\n",
    "    \n",
    "    def find_max_tf(self, doc):\n",
    "        max_tf = 0\n",
    "        max_term = None\n",
    "        for term, tf in doc.items():\n",
    "            if tf > max_tf:\n",
    "                max_tf = tf\n",
    "                max_term = term\n",
    "        return max_tf\n",
    "\n",
    "    \n",
    "    \n",
    "    def create_doc(self, docId):\n",
    "        self.docs[docId] = dict.fromkeys(self.vocab, 0)\n",
    "        \n",
    "    def add_term(self, term, docId, position):\n",
    "        if term not in self.vocab.keys():\n",
    "            self.vocab[term] = 1\n",
    "            for Id, docList in self.docs.items():\n",
    "                self.docs[Id][term] = 0\n",
    "        else:\n",
    "            self.vocab[term]+=1\n",
    "        \n",
    "        if term in self.docs[docId].keys():\n",
    "            self.docs[docId][term] += 1\n",
    "        else:\n",
    "            self.docs[docId][term] = 1\n",
    "            \n",
    "        if docId not in self.occurrance.keys():\n",
    "            self.occurrance[docId] = {}\n",
    "            self.occurrance[docId][term] = []\n",
    "            self.occurrance[docId][term].append(position)\n",
    "        else:\n",
    "            if term not in self.occurrance[docId].keys():\n",
    "                self.occurrance[docId][term] = []\n",
    "                self.occurrance[docId][term].append(position)\n",
    "            else:\n",
    "                self.occurrance[docId][term].append(position)\n",
    "                \n",
    "        if term in self.index.keys():\n",
    "            self.index[term].add(docId)\n",
    "        else:\n",
    "            self.index[term] = set()\n",
    "            self.index[term].add(docId)\n",
    "#         if docId not in self.occurrance.keys():\n",
    "#             self.occurrance[docId] = []\n",
    "#         self.occurrance[docId].append(position)\n",
    "    \n",
    "    def get_query_vector(self, query_terms):\n",
    "        query_vector_hash = dict.fromkeys(self.vocab, 0)\n",
    "#         print(query_vector_hash)\n",
    "        query_terms = [lem.lemmatize(word.lower()) for word in query_terms]\n",
    "        print(query_terms)\n",
    "        for term in query_terms:\n",
    "            if term in query_vector_hash.keys():\n",
    "                query_vector_hash[term] += 1\n",
    "\n",
    "        \n",
    "        words_in_query = len(query_terms)\n",
    "        tf = dict.fromkeys(self.vocab_idf, 0) \n",
    "        tf_idf = dict.fromkeys(self.vocab_idf, 0)\n",
    "        \n",
    "        for term,term_cnt in query_vector_hash.items():\n",
    "            if term_cnt <= 0:\n",
    "                continue\n",
    "#                 print(term)\n",
    "#                 print(term_cnt)\n",
    "#                 tf[term] = term_cnt / words_in_query\n",
    "#                 idf[term] = len(self.docs.keys()) / (self.vocab[term])\n",
    "\n",
    "            tf[term] = self.tf_func( term_cnt,query_vector_hash )\n",
    "            tf_idf[term] = tf[term] *  self.vocab_idf[term]\n",
    "            print(f'tf_idf {term}: {tf_idf[term]}')\n",
    "        new_query_vector_hash = {\n",
    "            'tf': tf,\n",
    "            'idf':self.vocab_idf,\n",
    "            'tf_idf':tf_idf\n",
    "        }\n",
    "    \n",
    "    \n",
    "        return new_query_vector_hash\n",
    "    \n",
    "    def dot_product(self, v_hash_1, v_hash_2):\n",
    "#         print(len(v_hash_1))\n",
    "#         print(len(v_hash_2))\n",
    "#         print('vhash')\n",
    "#         print(v_hash_2)\n",
    "        return sum([v_hash_2[x]*y for x,y in v_hash_1.items()])\n",
    "        \n",
    "    def get_magnitude(self, v_hash):\n",
    "        \n",
    "        mag = sum([x**2 for x in v_hash.values()]) ** 0.5 \n",
    "        if mag == 0:\n",
    "            return 1\n",
    "        return mag\n",
    "    \n",
    "    \n",
    "    def get_cosine_sim(self, v_hash_x, v_hash_y):\n",
    "        return (self.dot_product(v_hash_x,v_hash_y)) / (self.get_magnitude(v_hash_x) * self.get_magnitude(v_hash_y)) \n",
    "            \n",
    "    def get_ranking(self, query_vector_hash):\n",
    "        ranked_docs = []\n",
    "        for docId,doc_vector_hash in self.cdocs.items():\n",
    "            cosine_sim = self.get_cosine_sim(doc_vector_hash['tf_idf'], query_vector_hash['tf_idf'])\n",
    "            ranked_docs.append((docId, cosine_sim))\n",
    "        ranked_docs = sorted(ranked_docs, key=lambda x:x[1])\n",
    "        return ranked_docs\n",
    "    \n",
    "    def calculate_tf_idf(self):\n",
    "        cdocs = {}\n",
    "        self.vocab_idf = dict.fromkeys(self.vocab, 0)\n",
    "        for term, term_cnt in self.vocab.items():\n",
    "            self.vocab_idf[term] = self.idf_func(term)\n",
    "            \n",
    "        for docId,doc in self.docs.items():\n",
    "#             print(doc.values())\n",
    "            words_in_d = sum(doc.values())\n",
    "            tf = {}\n",
    "        \n",
    "            tf_idf = {}\n",
    "            for term,term_cnt in doc.items():\n",
    "#                 tf[term] = term_cnt / words_in_d\n",
    "#                 idf[term] = len(self.docs.keys()) / (self.vocab[term])\n",
    "                tf[term] =self.tf_func(term_cnt, doc)\n",
    "                tf_idf[term] = tf[term] * self.vocab_idf[term]\n",
    "            cdocs[docId] = {\n",
    "                'tf': tf,\n",
    "                'tf_idf':tf_idf,\n",
    "                'total_words' : words_in_d\n",
    "            }\n",
    "        self.cdocs = cdocs\n",
    "        return self.cdocs\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_space = VectorSpaceModel()\n",
    "\n",
    "d1= \"Music is a universal language\"\n",
    "d2= \"Music is a miracle\"\n",
    "d3= \"Music is a universal feature of the human experience\"\n",
    "# test_space.create_doc(1)\n",
    "# for word in d1.split(' '):\n",
    "#     test_space.add_term(word, 1)\n",
    "# test_space.create_doc(2)\n",
    "# for word in d2.split(' '):\n",
    "#     test_space.add_term(word, 2)\n",
    "# test_space.create_doc(3)\n",
    "# for word in d3.split(' '):\n",
    "#     test_space.add_term(word, 3)\n",
    "# test_space.calculate_tf_idf()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Punctuation\n",
    "def remove_punctuation(word):\n",
    "    return word.translate(word.maketrans('','',string.punctuation))\n",
    "\n",
    "# Clean Query Term\n",
    "def clean_word(word):\n",
    "    # Case Folding\n",
    "    word = word.lower()\n",
    "     # Filter non-ASCII characters\n",
    "    word = ''.join(filter(lambda x: x in printable, word))\n",
    "#     print(word)\n",
    "    # Remove Punctuations\n",
    "    if word != '(' and word != ')':\n",
    "        word = remove_punctuation(word)\n",
    "#     print(word)\n",
    "    if re.match('\\d+[A-Za-z]+',word):\n",
    "        word = re.split('\\d+',word)[1]\n",
    "    if re.match('[A-Za-z]+\\d+',word):\n",
    "        word = re.split('\\d+',word)[0]\n",
    "#     print(word)\n",
    "    word = ps.stem(word)\n",
    "#     print(word)\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term - Document Indexing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading : ********************************************************Done\n"
     ]
    }
   ],
   "source": [
    "vocab = set()\n",
    "doc_contents = []\n",
    "\n",
    "vector_space = VectorSpaceModel(tf_func='natural',idf_func='idf')\n",
    "printable = set(string.printable) \n",
    "raw_data = []\n",
    "# Printable characters are\n",
    "# 0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\n",
    "# !\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~ \\t\\n\\r\\x0b\\x0c\n",
    "\n",
    "\n",
    "lem = WordNetLemmatizer() \n",
    "\n",
    "stop_words = set()\n",
    "with open('Stopword-List.txt', 'r') as stop_word_file:\n",
    "    lines = stop_word_file.readlines()\n",
    "    for line in lines:\n",
    "        stop_words.add(line.split('\\n')[0])\n",
    "    stop_words.remove('')\n",
    "\n",
    "    \n",
    "print('Loading : ', end='')\n",
    "for file_number in range(0, 56):\n",
    "    vector_space.create_doc(file_number)\n",
    "    with open(f'data/Trump Speechs/speech_{file_number}.txt', 'r') as file1:\n",
    "        lines = file1.readlines()\n",
    "#         print(f'File Number : speech_{file_number}.txt' )\n",
    "#         print(lines[0])\n",
    "        position = {'doc':file_number,'row':0, 'col':0, 'token_no':0}\n",
    "        \n",
    "#         {\n",
    "#             'total_count' : 0,\n",
    "#             'postings' : {\n",
    "#                 'count':0,\n",
    "#                 'doc_id':0,\n",
    "#                 'positions':[]\n",
    "#             }\n",
    "#         }\n",
    "        \n",
    "        for line_no,line in enumerate(lines):\n",
    "            # Skip Heading Line\n",
    "            if line_no == 0:\n",
    "                continue\n",
    "            doc_set = set()\n",
    "            # split words at . , whitespace ? ! : ;\n",
    "            position['row'] = line_no \n",
    "            position['col'] = 0\n",
    "            symbols = \"!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~\\n\"\n",
    "            for i in symbols:\n",
    "                line = line.replace(i, ' ')\n",
    "            raw_data.append(line)\n",
    "            for word in re.split('[.\\s,?!:;-]', line):\n",
    "                \n",
    "                position['col'] += len(word) + 1\n",
    "                position['token_no'] += 1\n",
    "                # Case Folding\n",
    "                word = word.lower()\n",
    "                \n",
    "                # Filter non-ASCII characters\n",
    "                word = ''.join(filter(lambda x: x in printable, word))\n",
    "                \n",
    "                # Remove Punctuations\n",
    "                word = remove_punctuation(word)\n",
    "                \n",
    "                if re.match('\\d+[A-Za-z]+',word):\n",
    "                    word = re.split('\\d+',word)[1]\n",
    "                if re.match('[A-Za-z]+\\d+',word):\n",
    "                    word = re.split('\\d+',word)[0]\n",
    "                \n",
    "                if len(word) == 0 or len(word) == 1 or word == '' or word == ' ':\n",
    "                    continue\n",
    "                if word in stop_words:\n",
    "                    continue\n",
    "                \n",
    "                word = lem.lemmatize(word)\n",
    "                    \n",
    "                vocab.add(word)\n",
    "                \n",
    "                doc_set.add(word)\n",
    "                \n",
    "                vector_space.add_term(word, file_number, deepcopy(position))\n",
    "                \n",
    "        doc_contents.append(doc_set)\n",
    "        print('*', end='')\n",
    "doc_term_tf_idf = vector_space.calculate_tf_idf()\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Vocabulary Size \n",
      "6249\n",
      "Total Number of Documents \n",
      "56\n"
     ]
    }
   ],
   "source": [
    "print('Total Vocabulary Size ')\n",
    "print(len(vector_space.index.keys()))\n",
    "print('Total Number of Documents ')\n",
    "print(len(vector_space.docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To save Inverted Index in File\n",
    "with open('pickled/vector_space.p', 'wb') as index_file:\n",
    "    pickle.dump(vector_space, index_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hillary', 'clinton']\n",
      "['hillary', 'clinton']\n",
      "tf_idf clinton: 0.15657531416991727\n",
      "tf_idf hillary: 0.15871020592607524\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{1, 2, 9, 16, 20, 31, 32, 36, 40, 42, 43, 44, 46, 52}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Hillary clinton\"\n",
    "query_terms = [lem.lemmatize(x) for x in query.split(' ')]\n",
    "print(query_terms)\n",
    "\n",
    "query_vector = vector_space.get_query_vector(query_terms)\n",
    "\n",
    "# cs = vector_space.get_cosine_sim(vector_space.docs[1], query_vector)\n",
    "# print([(x,y) for x,y in query_vector.items() if y > 0])\n",
    "# ranked = vector_space.get_ranking(vector_space.get_query_vector(query_terms))\n",
    "# ranked.reverse()\n",
    "# ranked = set([x for x,y in ranked if y > 0.0005])\n",
    "# print(ranked)\n",
    "\n",
    "vector_space.index['global']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t['massive', 'inflow', 'of', 'refugee']\n",
      "['massive', 'inflow', 'of', 'refugee']\n",
      "tf_idf of: 0.0752574989159953\n",
      "tf_idf massive: 0.09666474956558715\n",
      "tf_idf refugee: 0.11203938969608647\n",
      "tf_idf inflow: 0.18833191666465288\n",
      "[(41, 0.14238395203047455), (54, 0.13614270772046327), (48, 0.13577266734153465), (29, 0.13318415662080998), (47, 0.13296037933069274), (40, 0.13285554788949944), (7, 0.12825833096070022), (49, 0.12809037874567442), (39, 0.12617821737443477), (32, 0.12610667649337276), (24, 0.12493493864380238), (52, 0.12444365156394711), (20, 0.12361639684231786), (46, 0.12338069327849539), (12, 0.12282088533663452), (14, 0.11852880246109719), (4, 0.11848620892779728), (16, 0.1183355922760708), (18, 0.11779896665405311), (25, 0.11775128353511148), (37, 0.11774530330133837), (9, 0.11722407323682882), (21, 0.11522891772785722), (3, 0.11379539932382887), (30, 0.11362809063388735), (17, 0.10805046214024351), (50, 0.10510484882625705), (11, 0.10474089871210134), (22, 0.10385616011878274), (27, 0.10352652513169272), (45, 0.10259182403798843), (38, 0.09912932170015308), (26, 0.0986998365073861), (44, 0.09525419954044391), (33, 0.09401461817455435), (43, 0.09363368220213263), (31, 0.09087258878267038), (53, 0.09045939423947945), (10, 0.09027377263223339), (34, 0.08910371993657006), (36, 0.0886153938757195), (42, 0.0871349786077251), (13, 0.08690494928907043), (2, 0.08408158433360509), (5, 0.08320053809366933), (51, 0.0750210926154078), (35, 0.07479781276791421), (28, 0.07351003232955824), (6, 0.0703208414614081), (1, 0.060555947131281096), (19, 0.059458798749836145), (23, 0.058279597514305795), (0, 0.056886525736934375), (8, 0.05144697144756244), (15, 0.0510620916751955), (55, 0.037605894166514144)]\n",
      "Normal results\n",
      "results length :  56\n",
      "Result Differnece : \n",
      "set()\n",
      "Symmetric Difference : \n",
      "{'34', '24', '3', '22', '53', '51', '17', '35', '23', '15', '19', '18', '16', '9', '1', '25', '13', '21', '27', '28', '33', '6', '5', '14', '42', '55', '11', '2', '26', '8', '0', '45', '10', '7', '36', '4', '43'}\n",
      "\n",
      "precision : 0.3392857142857143\n",
      "recall : 1.0\n",
      "\t['pakistan', 'afghanistan']\n",
      "['pakistan', 'afghanistan']\n",
      "tf_idf afghanistan: 0.37666383332930575\n",
      "tf_idf pakistan: 0.8779374278362457\n",
      "[(3, 0.031685006884602025), (22, 0.0064499230979523986), (16, 0.006252201704354322), (41, 0.00621339228308768), (40, 0.005214557003612471), (39, 0.0050518594823124), (17, 0.004978832166081623), (18, 0.004756600924779354), (4, 0.004157133489485907), (37, 0.004137007712287927), (1, 0.003986437335774213), (9, 0.002541584499871822)]\n",
      "Normal results\n",
      "results length :  12\n",
      "Result Differnece : \n",
      "set()\n",
      "Symmetric Difference : \n",
      "{'41', '18', '39', '40', '37', '9'}\n",
      "\n",
      "precision : 0.5\n",
      "recall : 1.0\n",
      "\t['hillary', 'clinton']\n",
      "['hillary', 'clinton']\n",
      "tf_idf clinton: 0.15657531416991727\n",
      "tf_idf hillary: 0.15871020592607524\n",
      "[(4, 0.3057970312575895), (36, 0.29918282813915137), (42, 0.2438681003807444), (33, 0.24238302866262723), (35, 0.22147312418915577), (20, 0.21833585303827027), (34, 0.21520653872424517), (12, 0.2140308959186334), (29, 0.1861501716778399), (16, 0.18283450919611136), (21, 0.1814602013973697), (14, 0.1741553536973717), (47, 0.16956707291467896), (22, 0.1693821857784422), (46, 0.16646776881730393), (53, 0.15893847756718388), (41, 0.1491159981676003), (11, 0.14468354594579816), (54, 0.1402618632660213), (45, 0.13724176934749124), (51, 0.13503865788065794), (40, 0.13286699020443454), (37, 0.12089532319470736), (39, 0.11729420501886398), (3, 0.11510480731191966), (25, 0.10533045790147245), (10, 0.10512282602821156), (17, 0.10065004883745847), (48, 0.09416239475653822), (49, 0.0895910489428229), (44, 0.08939145384659641), (18, 0.08559153191992712), (5, 0.08125102407727058), (28, 0.08007723189367298), (30, 0.06932859770358443), (32, 0.06867744464273916), (24, 0.06192545893754915), (8, 0.05579633872216815), (43, 0.05511519976655602), (52, 0.05048811981425103), (9, 0.049446071676236276), (50, 0.04581356368835888), (26, 0.044324442169553877), (7, 0.044090774000947255), (31, 0.038638212155891014), (6, 0.03851349388471189), (19, 0.035433415598542364), (27, 0.03229918524742794), (2, 0.023438904184509047), (38, 0.011760623847619004), (55, 0.007147002448199362), (1, 0.00598450392514668), (0, 0.0013639321600313154)]\n",
      "Normal results\n",
      "results length :  53\n",
      "Result Differnece : \n",
      "set()\n",
      "Symmetric Difference : \n",
      "{'0'}\n",
      "\n",
      "precision : 0.9811320754716981\n",
      "recall : 1.0\n",
      "\t['personnel', 'policy']\n",
      "['personnel', 'policy']\n",
      "tf_idf policy: 0.1810833695089817\n",
      "tf_idf personnel: 0.5880456295278407\n",
      "[(5, 0.052882698198823505), (42, 0.04097517485819756), (18, 0.031655664531949554), (2, 0.031363081578510206), (25, 0.020170038129752488), (11, 0.019734412036912313), (24, 0.018237812286082324), (4, 0.017901633928245224), (29, 0.016830812094618952), (17, 0.016080052105517235), (10, 0.015077263295314348), (33, 0.014216824445746448), (22, 0.013887473478648884), (3, 0.013767800493404068), (27, 0.012926453767870872), (23, 0.012292610396004154), (20, 0.010452443121896806), (21, 0.01021439834432997), (26, 0.009915899874409304), (51, 0.007456978527227093), (52, 0.007419214182257744), (31, 0.007408991343519859), (16, 0.006730877565032387), (54, 0.006254826198963559), (49, 0.005695514265479198), (40, 0.00561378957475947), (36, 0.005508979694794445), (39, 0.005438635741311911), (7, 0.004790703391444443), (9, 0.004560285293287731), (48, 0.0044956656984346055), (45, 0.004107772053165618), (44, 0.0029765422870737745), (37, 0.002969161234578825), (13, 0.0029148455339385567), (6, 0.002909339858026938), (14, 0.0028777844045638876), (19, 0.002733476364507237), (43, 0.00240430672599285), (34, 0.0022393066922203426), (32, 0.002059070312502024), (12, 0.0019390784029245066), (8, 0.0007964397361391613)]\n",
      "Normal results\n",
      "results length :  43\n",
      "Result Differnece : \n",
      "set()\n",
      "Symmetric Difference : \n",
      "{'34', '44', '40', '24', '3', '37', '32', '51', '48', '23', '54', '19', '52', '39', '16', '9', '13', '21', '33', '6', '31', '20', '12', '42', '14', '2', '26', '8', '49', '45', '7', '36', '4', '43'}\n",
      "\n",
      "precision : 0.20930232558139536\n",
      "recall : 1.0\n",
      "\t['united', 'plane']\n",
      "['united', 'plane']\n",
      "tf_idf united: 0.16090680079363665\n",
      "tf_idf plane: 0.316436969371091\n",
      "[(25, 0.04094722759956611), (1, 0.036937360051127965), (29, 0.03246851205507999), (41, 0.027462419499632042), (17, 0.02657192269782506), (48, 0.021533375403554027), (24, 0.02142539511196655), (3, 0.020934958909700426), (35, 0.020269195885157007), (36, 0.019771263370299975), (38, 0.01923643827382551), (33, 0.019026171868926276), (2, 0.018657642062874465), (34, 0.017980991812627276), (27, 0.017221131581215823), (26, 0.017190293937384003), (20, 0.01668839846539834), (32, 0.016533745270064903), (49, 0.015588811220367269), (52, 0.014893541596018453), (39, 0.01488572619008625), (30, 0.014808060224281177), (4, 0.014290898151380538), (40, 0.012804273362139244), (19, 0.012520420817557512), (13, 0.011967043180061915), (46, 0.011261726024571456), (0, 0.008926346684897796), (9, 0.008737156341661606), (28, 0.008731967217727218), (45, 0.008432331141002539), (37, 0.008126692658680861), (12, 0.007960982737211563), (31, 0.007604495529545621), (50, 0.007493563278732962), (21, 0.006989286660402862), (51, 0.006803332418978972), (5, 0.00665247924768179), (7, 0.006556156942196444), (47, 0.006341994141695252), (22, 0.006335079160415377), (44, 0.006110171132904184), (54, 0.005706555503561512), (55, 0.0046760400883155825), (43, 0.0032903335783505748), (16, 0.0030704391449676836), (8, 0.0027248524269186222), (6, 0.002654319856450293), (18, 0.0023359536955214547), (10, 0.002292609394720379), (11, 0.002152723105577155)]\n",
      "Normal results\n",
      "results length :  51\n",
      "Result Differnece : \n",
      "set()\n",
      "Symmetric Difference : \n",
      "{'6', '0'}\n",
      "\n",
      "precision : 0.9607843137254902\n",
      "recall : 1.0\n",
      "\t['develop', 'solution']\n",
      "['develop', 'solution']\n",
      "tf_idf solution: 0.316436969371091\n",
      "tf_idf develop: 0.40977196777093433\n",
      "[(23, 0.08922182954244356), (38, 0.05132546877752921), (2, 0.030538364856831323), (21, 0.027801681617986625), (32, 0.027475862005527183), (17, 0.02174613135891305), (18, 0.020775488122867444), (3, 0.01486117081822314), (16, 0.013653924756848313), (30, 0.013385207680262136), (39, 0.013158177024588871), (9, 0.011100922575883404), (20, 0.010601654132965124), (27, 0.009382237112470045), (35, 0.009160801358366683), (25, 0.009149845140206886), (51, 0.009020666964739901), (33, 0.008599008174222284), (24, 0.008273318921874715), (40, 0.006790971166020609), (52, 0.006731237729248462), (31, 0.006721962844318326), (5, 0.005880431930267394), (1, 0.00519157830351385), (8, 0.0024234272854187417)]\n",
      "Normal results\n",
      "results length :  25\n",
      "Result Differnece : \n",
      "set()\n",
      "Symmetric Difference : \n",
      "{'31', '8', '1', '25'}\n",
      "\n",
      "precision : 0.84\n",
      "recall : 1.0\n",
      "\t['development', 'praised']\n",
      "['development', 'praised']\n",
      "tf_idf development: 0.731198998949478\n",
      "tf_idf praised: 0.731198998949478\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(31, 0.035939866291364315), (36, 0.017815469920515203), (44, 0.014438744337525554), (8, 0.003863405462203211)]\n",
      "Normal results\n",
      "results length :  4\n",
      "Result Differnece : \n",
      "set()\n",
      "Symmetric Difference : \n",
      "{'31'}\n",
      "\n",
      "precision : 0.75\n",
      "recall : 1.0\n",
      "\t['muslim']\n",
      "['muslim']\n",
      "tf_idf muslim: 0.9542425094393249\n",
      "[(3, 0.06847745290157702), (4, 0.040067128561783606), (9, 0.03266160626131962), (2, 0.022462818788261157), (20, 0.01559631871326888), (6, 0.008682179111972119), (7, 0.008577975818554469)]\n",
      "Normal results\n",
      "results length :  7\n",
      "Result Differnece : \n",
      "set()\n",
      "Symmetric Difference : \n",
      "{'2', '20', '9', '7', '6'}\n",
      "\n",
      "precision : 0.2857142857142857\n",
      "recall : 1.0\n",
      "\t['american', 'energy', 'revolution']\n",
      "['american', 'energy', 'revolution']\n",
      "tf_idf american: 0.10165342976413785\n",
      "tf_idf energy: 0.12466262584883647\n",
      "tf_idf revolution: 0.3180808364797749\n",
      "[(31, 0.14087897587989007), (30, 0.08667228563724078), (13, 0.07443312059301091), (33, 0.07350047489759047), (34, 0.07311723865743368), (27, 0.07069400291502571), (35, 0.06800374374398314), (32, 0.06641196040792244), (52, 0.06401217716038728), (12, 0.062173931496487346), (24, 0.06047824690048675), (25, 0.05967713310971785), (21, 0.05835905594081183), (36, 0.05396567335147879), (26, 0.05321554591651567), (14, 0.05288050749091076), (29, 0.05102549055660531), (11, 0.050705362285854795), (18, 0.04986088048219195), (28, 0.041651195007631), (10, 0.03964180749953206), (49, 0.03835004157717275), (51, 0.037530221205483716), (46, 0.03695757553509481), (54, 0.0357397383163668), (16, 0.03570656491358469), (48, 0.028436840057438077), (40, 0.028253637015648748), (39, 0.026638293854182333), (7, 0.02502427118539658), (4, 0.024958819441709936), (20, 0.02488156218146539), (41, 0.024568686705575583), (22, 0.024552221037075123), (43, 0.02453081332762214), (23, 0.024088811955038286), (37, 0.021814309662172356), (47, 0.021750415838147494), (50, 0.021231815360140766), (3, 0.021220050681027624), (17, 0.021156393257136016), (45, 0.02095551876539744), (42, 0.020271393881288315), (2, 0.018425813239392526), (5, 0.017188224150822787), (9, 0.016217218069734723), (15, 0.011037423292097719), (19, 0.010930915940716445), (44, 0.00971510478073885), (55, 0.006970173956997009), (1, 0.004669150033954509), (53, 0.0036327329630494436), (8, 0.002438300953919936), (6, 0.0007913136238160176), (0, 0.0005393787540910795)]\n",
      "Normal results\n",
      "results length :  55\n",
      "Result Differnece : \n",
      "set()\n",
      "Symmetric Difference : \n",
      "{'6', '0'}\n",
      "\n",
      "precision : 0.9636363636363636\n",
      "recall : 1.0\n",
      "\t['future', 'of', 'new', 'america']\n",
      "['future', 'of', 'new', 'america']\n",
      "tf_idf of: 0.0752574989159953\n",
      "tf_idf new: 0.08045340039681832\n",
      "tf_idf america: 0.0752574989159953\n",
      "tf_idf future: 0.09349696938662735\n",
      "[(14, 0.2859924507697716), (20, 0.2821063644130835), (12, 0.2495227249728828), (54, 0.2487030480167417), (21, 0.23940413875577793), (41, 0.2380373371512802), (7, 0.23389905428234964), (26, 0.23349704851509356), (18, 0.23236734537470405), (33, 0.22929393300171902), (24, 0.22621620499017056), (52, 0.22539912661893072), (40, 0.22250934995060365), (25, 0.2224586366567846), (27, 0.21847192993242356), (4, 0.2151470067401978), (30, 0.2149598494492198), (17, 0.21431828383247725), (10, 0.2138897742765686), (11, 0.21256617684795942), (16, 0.2084166055184674), (29, 0.20820286235435567), (32, 0.20715718311851067), (35, 0.20531760649052994), (34, 0.20475818567735302), (13, 0.20459654540006472), (3, 0.20351389166511474), (39, 0.20272754923515401), (31, 0.20209412048326533), (48, 0.20101157292964156), (43, 0.19941746011792605), (37, 0.1993324708633134), (51, 0.19781633024546072), (47, 0.19692622700008272), (45, 0.19254713005478924), (9, 0.19188156544143325), (46, 0.190182043116398), (49, 0.18948033616950416), (22, 0.18713654220075177), (44, 0.1844090442937927), (42, 0.1785938739128978), (36, 0.17756044925411912), (2, 0.17508127386238476), (50, 0.17400245252569999), (38, 0.16756438228571277), (5, 0.16096533812765473), (53, 0.14520750547347375), (23, 0.12033044936381715), (15, 0.11722193384176008), (6, 0.1158574802696793), (28, 0.10821722990838065), (1, 0.10766292697109076), (19, 0.10360590586990549), (0, 0.09806732049562988), (8, 0.08290273658933614), (55, 0.07151830907739926)]\n",
      "Normal results\n",
      "results length :  56\n",
      "Result Differnece : \n",
      "set()\n",
      "Symmetric Difference : \n",
      "{'53', '28', '0'}\n",
      "\n",
      "precision : 0.9464285714285714\n",
      "recall : 1.0\n",
      "\t['hillary', 'clinton', 'is', 'the', 'worst', 'looser']\n",
      "['hillary', 'clinton', 'is', 'the', 'worst', 'looser']\n",
      "tf_idf is: 0.0752574989159953\n",
      "tf_idf worst: 0.1307196863200844\n",
      "tf_idf clinton: 0.07828765708495863\n",
      "tf_idf hillary: 0.07935510296303762\n",
      "[(4, 0.252752277197588), (36, 0.24440915217485995), (42, 0.20929183209964533), (12, 0.1931310598848338), (20, 0.19294043736484862), (34, 0.19134834481349458), (33, 0.1897066431403722), (53, 0.18527811564092056), (35, 0.18026131482003738), (29, 0.16785715065637827), (47, 0.16647178785528632), (41, 0.16447281930078944), (16, 0.15697519164960635), (54, 0.15664903863333848), (40, 0.15322555915648928), (37, 0.15260804189688476), (14, 0.14916511490506856), (3, 0.14474858604465543), (21, 0.14257565475436196), (39, 0.14165312930445556), (44, 0.14083102143201062), (51, 0.13941321720933264), (46, 0.13940288620261787), (10, 0.13275466692353846), (11, 0.13233995187021852), (48, 0.1298810964862097), (22, 0.12952377139014318), (17, 0.12262192486275439), (18, 0.1224420598075518), (49, 0.11784686247794442), (45, 0.11652156198846195), (5, 0.11072398965544211), (25, 0.10836620745775166), (28, 0.10543958351791721), (52, 0.09726949646352039), (30, 0.09554183735183742), (7, 0.08596145981920222), (9, 0.08113756037657525), (32, 0.07819256334905682), (31, 0.07757540537727077), (24, 0.0756012702995924), (43, 0.07362729817651344), (8, 0.07243890613539594), (26, 0.06808183257013203), (50, 0.06756330656774652), (2, 0.062316389721786575), (19, 0.06073122490326987), (1, 0.06032511556753212), (6, 0.059175860439531595), (55, 0.05652175669257936), (27, 0.051424202041310874), (15, 0.04713786846279114), (38, 0.04522029795020251), (0, 0.03978959556657351), (23, 0.025074966607145983), (13, 0.019819401541231768)]\n",
      "Normal results\n",
      "results length :  56\n",
      "Result Differnece : \n",
      "set()\n",
      "Symmetric Difference : \n",
      "{'15', '23', '13', '0'}\n",
      "\n",
      "precision : 0.9285714285714286\n",
      "recall : 1.0\n",
      "\t['no', 'patience', 'for', 'injustice']\n",
      "['no', 'patience', 'for', 'injustice']\n",
      "tf_idf patience: 0.731198998949478\n",
      "tf_idf injustice: 0.5431799153373741\n",
      "[(11, 0.026885894554304783), (7, 0.016376295100164175), (22, 0.014067812503800684), (16, 0.013636565890331607), (15, 0.012640690055526803)]\n",
      "Normal results\n",
      "results length :  5\n",
      "Result Differnece : \n",
      "set()\n",
      "Symmetric Difference : \n",
      "set()\n",
      "\n",
      "precision : 1.0\n",
      "recall : 1.0\n",
      "\t['global', 'interest']\n",
      "['global', 'interest']\n",
      "tf_idf interest: 0.17292117004017174\n",
      "tf_idf global: 0.34948500216800943\n",
      "[(42, 0.07279347903612585), (44, 0.07194027552504498), (36, 0.05351866357533877), (2, 0.027230623942445322), (34, 0.02255556209075541), (35, 0.021793638419383504), (43, 0.021050939076742978), (33, 0.020457126793079657), (20, 0.020266222485314584), (46, 0.02006972812688025), (50, 0.019697919842233777), (16, 0.01964409560320719), (32, 0.018028217129954956), (10, 0.016874079305071493), (49, 0.01639098209664703), (40, 0.013691237986772197), (11, 0.013580989101895957), (52, 0.01357080974777343), (31, 0.013552110705504805), (48, 0.01293796706333979), (12, 0.011160853227590459), (4, 0.010733057760511875), (9, 0.010721481391060387), (37, 0.010681096203422731), (0, 0.009511750091877406), (28, 0.009181297811040117), (1, 0.008408241731543272), (14, 0.008281905804207136), (26, 0.008153350651441515), (51, 0.007153419096597119), (22, 0.006661070406930217), (3, 0.00660366974425757), (53, 0.006406193761428152), (54, 0.006000204106018782), (7, 0.005514819548110136), (30, 0.005307257246397324), (6, 0.004186359238446131), (38, 0.004045261834174692), (21, 0.0036744710965939255), (25, 0.003627928911060819), (23, 0.0035376604733165186), (45, 0.002955414341977762), (17, 0.002570910621638803), (55, 0.002458330153829998), (18, 0.0024561574747831577), (5, 0.0023316011017056274), (8, 0.002292054562173399), (19, 0.00078666052025903)]\n",
      "Normal results\n",
      "results length :  48\n",
      "Result Differnece : \n",
      "{'41', '39', '24', '47', '27'}\n",
      "Symmetric Difference : \n",
      "{'41', '34', '44', '53', '3', '37', '30', '35', '32', '51', '23', '17', '19', '52', '24', '39', '38', '9', '1', '47', '28', '27', '33', '6', '31', '20', '12', '5', '14', '55', '2', '26', '49', '0', '45', '50', '46', '7', '36', '4', '43'}\n",
      "\n",
      "precision : 0.32075471698113206\n",
      "recall : 0.7727272727272727\n",
      "\t['pakistan', 'afghanistan', 'aid']\n",
      "['pakistan', 'afghanistan', 'aid']\n",
      "tf_idf afghanistan: 0.25110922221953713\n",
      "tf_idf pakistan: 0.5852916185574971\n",
      "tf_idf aid: 0.3380801463715367\n",
      "[(3, 0.02798634809664383), (22, 0.016023695753849307), (42, 0.015834308839983997), (41, 0.015436076683630383), (40, 0.012954646690828957), (39, 0.012550453409509778), (29, 0.010727437954144547), (16, 0.005522368794356272), (17, 0.004397642412457302), (18, 0.0042013527004276996), (4, 0.0036718623841457162), (37, 0.0036540859320707804), (1, 0.0035210919584383058), (9, 0.0022448998919111524)]\n",
      "Normal results\n",
      "results length :  14\n",
      "Result Differnece : \n",
      "set()\n",
      "Symmetric Difference : \n",
      "{'18', '37', '9'}\n",
      "\n",
      "precision : 0.7857142857142857\n",
      "recall : 1.0\n",
      "\t['biggest', 'plane', 'wanted', 'hour']\n",
      "['biggest', 'plane', 'wanted', 'hour']\n",
      "tf_idf wanted: 0.18833191666465288\n",
      "tf_idf plane: 0.1582184846855455\n",
      "tf_idf biggest: 0.11203938969608647\n",
      "tf_idf hour: 0.16330312844383593\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.027337808441140084), (8, 0.026750921228480887), (52, 0.021981219420626547), (35, 0.021915179594799127), (19, 0.017810235973704694), (51, 0.014185321005353902), (24, 0.01400687911406263), (34, 0.013758535352917368), (43, 0.013721051728529965), (41, 0.013718731214653206), (11, 0.011615738674330958), (40, 0.011513373480500303), (30, 0.010988317196687901), (2, 0.010891490037294584), (9, 0.01057930201176322), (5, 0.010270483707070744), (28, 0.010110688478088246), (1, 0.008523843710632039), (39, 0.008449755468759684), (36, 0.0082022821961308), (46, 0.008183984281999155), (27, 0.007691012404753345), (25, 0.007500510979558549), (29, 0.007152883477022979), (33, 0.007048966866187816), (53, 0.006641393000257859), (15, 0.0065912060769672), (3, 0.006448067638902592), (26, 0.006327306649801213), (54, 0.006220497698612816), (32, 0.006125552916560202), (6, 0.0055344023922151795), (55, 0.005414356635241327), (17, 0.005315193273571361), (10, 0.005309195239509735), (42, 0.005294333290353419), (18, 0.005092667399095133), (4, 0.004450845999146655), (44, 0.004440309096750176), (37, 0.004429298282397144), (49, 0.004248191586846229), (50, 0.0040842232324452), (47, 0.0034565825162315472), (48, 0.003353244028044972), (16, 0.0033469681707436146), (45, 0.003063920458018905), (7, 0.0014293231405608745)]\n",
      "Normal results\n",
      "results length :  47\n",
      "Result Differnece : \n",
      "set()\n",
      "Symmetric Difference : \n",
      "{'41', '34', '44', '40', '24', '53', '3', '29', '30', '35', '51', '15', '32', '37', '17', '48', '54', '18', '39', '16', '9', '25', '47', '28', '27', '33', '6', '5', '42', '55', '11', '2', '26', '8', '49', '0', '45', '10', '50', '46', '7', '36', '4', '43'}\n",
      "\n",
      "precision : 0.06382978723404255\n",
      "recall : 1.0\n",
      "\t['near', 'architect', 'box']\n",
      "['near', 'architect', 'box']\n",
      "tf_idf box: 0.25110922221953713\n",
      "tf_idf near: 0.30102999566398114\n",
      "tf_idf architect: 0.43124358564082726\n",
      "[(54, 0.034199931271303684), (24, 0.020475422780484892), (39, 0.016282415808157324), (53, 0.013557767636874545), (18, 0.012668404936617694), (51, 0.010878424682790752), (43, 0.010522386327468604), (46, 0.008353409987548552), (50, 0.00833755050491166), (17, 0.007819325887669185), (25, 0.007677978377022514), (23, 0.007486938494456103), (47, 0.007056282103909664), (11, 0.00688434276755186), (9, 0.006769081984001162), (45, 0.006254700124819252), (4, 0.0045429878823313235), (44, 0.004532232843870392), (6, 0.004244216931379041)]\n",
      "Normal results\n",
      "results length :  19\n",
      "Result Differnece : \n",
      "set()\n",
      "Symmetric Difference : \n",
      "{'6', '44', '53', '45', '50', '46', '25', '47'}\n",
      "\n",
      "precision : 0.5789473684210527\n",
      "recall : 1.0\n",
      "\t['peaceful', 'change']\n",
      "['peaceful', 'change']\n",
      "tf_idf change: 0.1654966095207122\n",
      "tf_idf peaceful: 0.40977196777093433\n",
      "[(10, 0.03362828817651754), (4, 0.03168067136802822), (14, 0.030557034856525667), (30, 0.028160193796638474), (11, 0.02767878492774412), (2, 0.02226493733123709), (7, 0.020347554915855916), (3, 0.01980130753858208), (48, 0.018639933084106865), (20, 0.018233098939420146), (17, 0.016893570504612536), (52, 0.015098971800530004), (32, 0.011972713713352784), (21, 0.011878571255024806), (33, 0.011022059678644433), (16, 0.01043666741395718), (41, 0.010371883671934586), (50, 0.009551707100321175), (35, 0.008806617335718777), (36, 0.008542034572654763), (39, 0.008432961656086781), (29, 0.008388411031158798), (22, 0.008075039650278051), (13, 0.006779488953747319), (46, 0.006379917414429842), (25, 0.005864056478515686), (51, 0.005781267305024508), (5, 0.0056530768163976786), (43, 0.005592053060960762), (53, 0.005177372951658115), (54, 0.004849259263741675), (40, 0.0043522745851211145), (31, 0.004308047749612004), (42, 0.004127257334940824), (8, 0.0037047962443382708), (37, 0.003452909520821664), (49, 0.0033117257500565414), (9, 0.0031819597380180315), (23, 0.002859074877968691), (47, 0.0026946179550008144), (34, 0.0026041439944584515), (15, 0.002418619890327114), (45, 0.0023885138109976506), (6, 0.0022555611953291), (12, 0.0022550012444936983), (55, 0.0019867791263696553), (18, 0.0019850232054361566), (19, 0.0019072955258935688), (26, 0.0016473485934160499)]\n",
      "Normal results\n",
      "results length :  49\n",
      "Result Differnece : \n",
      "set()\n",
      "Symmetric Difference : \n",
      "{'34', '40', '15', '19', '18', '9', '6', '55', '26'}\n",
      "\n",
      "precision : 0.8163265306122449\n",
      "recall : 1.0\n",
      "normal f1 : 0.04664975477747482\n"
     ]
    }
   ],
   "source": [
    "test = [(None, None) for x in range(0, 17)] \n",
    "test[0] =(\"massive inflow of refugees\",\n",
    "{'32', '50', '49', '47', '46', '29', '48', '54', '41', '40', '30', '39', '12', '52', '37', '44', '31', '38', '20'}\n",
    "         )\n",
    "test[1] =('pakistan afghanistan',\n",
    "{'3', '22', '16', '17', '4', '1'}\n",
    "         )\n",
    "test[2] =('Hillary Clinton',\n",
    "{'4', '12', '36', '42', '33', '20', '29', '35', '16', '22', '47', '34', '14', '21', '45', '46', '41', '11', '3', '51', '53', '40', '37', '17', '48', '39', '5', '28', '10', '18', '54', '44', '25', '43', '49', '24', '30', '32', '8', '50', '9', '26', '7', '19', '52', '31', '27', '6', '2', '38', '1', '55'}\n",
    "         )\n",
    "test[3] =('personnel policies',\n",
    "\n",
    "{'5', '18', '11', '25', '10', '22', '29', '27', '17'}\n",
    "         )\n",
    "\n",
    "test[4] =('united plane',\n",
    "{'38', '1', '25', '41', '48', '29', '3', '17', '49', '20', '4', '52', '13', '39', '46', '22', '40', '36', '28', '12', '45', '26', '2', '51', '50', '19', '24', '5', '47', '31', '35', '21', '37', '55', '9', '33', '44', '54', '34', '7', '32', '43', '30', '16', '27', '18', '11', '10', '8'}\n",
    "         )\n",
    "\n",
    "test[5] =('develop solutions',\n",
    "{'38', '23', '2', '17', '32', '18', '51', '16', '39', '35', '21', '27', '33', '20', '24', '5', '3', '9', '40', '52', '30'}\n",
    "         )\n",
    "\n",
    "test[6] =('developments praised',\n",
    "{'36', '44', '8'}\n",
    "         )\n",
    "test[7] =(\"muslims\",\n",
    "{'4', '3'}\n",
    "         )\n",
    "test[8] =('American Energy Revolution',\n",
    "\n",
    "{'13', '28', '35', '33', '51', '27', '12', '36', '34', '14', '21', '29', '31', '30', '25', '32', '26', '24', '11', '54', '49', '16', '46', '48', '10', '18', '4', '50', '42', '5', '23', '47', '20', '43', '37', '39', '41', '45', '2', '22', '3', '40', '7', '17', '52', '44', '53', '19', '9', '55', '1', '15', '8'}\n",
    "         )\n",
    "test[9] =('Future of new America',\n",
    "{'51', '14', '35', '26', '13', '50', '33', '4', '12', '31', '20', '32', '29', '34', '41', '27', '46', '30', '40', '25', '17', '42', '16', '24', '54', '2', '43', '21', '7', '18', '52', '49', '5', '47', '45', '10', '36', '38', '44', '11', '3', '39', '22', '9', '48', '37', '23', '15', '1', '19', '55', '6', '8'}\n",
    "         )\n",
    "test[10] =('Hillary clinton is the worst looser',\n",
    "{'4', '12', '36', '42', '33', '20', '29', '35', '16', '22', '47', '34', '14', '21', '45', '46', '41', '11', '3', '51', '53', '40', '37', '17', '48', '39', '5', '28', '10', '18', '54', '44', '25', '43', '49', '24', '30', '32', '8', '50', '9', '26', '7', '19', '52', '31', '27', '6', '2', '38', '1', '55'}\n",
    "          )\n",
    "test[11] =('no patience for injustice',\n",
    "{'11', '22', '16', '7', '15'}\n",
    "          )\n",
    "test[12] =('Global interests',\n",
    "{'10', '16', '27', '42', '21', '11', '25', '22', '47', '41', '48', '54', '18', '40', '24', '39', '8'}\n",
    "          )\n",
    "test[13] =('pakistan afghanistan aid',\n",
    "{'3', '22', '42', '29', '41', '16', '40', '39', '17', '4', '1'}\n",
    "          )\n",
    "test[14] =('biggest plane wanted hour',\n",
    "{'52', '1', '19'}\n",
    "          )\n",
    "test[15] =('near architect box',\n",
    "{'54', '23', '18', '24', '39', '43', '51', '4', '9', '17', '11'}\n",
    "          )\n",
    "test[16] =('peaceful change',\n",
    "{'14', '10', '4', '30', '3', '11', '20', '16', '50', '52', '48', '17', '7', '2', '22', '29', '13', '53', '41', '46', '32', '21', '39', '25', '33', '54', '42', '5', '36', '31', '43', '51', '49', '23', '47', '35', '37', '12', '45', '8'}\n",
    "         )\n",
    "cumulative_precision_normal = 0\n",
    "cumulative_recall_normal = 0 \n",
    "cumulative_precision_sciket = 0\n",
    "cumulative_recall_sciket = 0 \n",
    "cumulative_precision_group = 0\n",
    "cumulative_recall_group = 0 \n",
    "\n",
    "cnt = 0\n",
    "for t in test:\n",
    "    cnt+=1\n",
    "    normal_cnt = [0, 0 , 0]\n",
    "    # my custom model\n",
    "    query = t[0]\n",
    "    query_terms = [lem.lemmatize(x.lower()) for x in query.split(' ')]\n",
    "    print('\\t' + str(query_terms))\n",
    "    # cs = vector_space.get_cosine_sim(vector_space.docs[1], query_vector)\n",
    "    # print([(x,y) for x,y in query_vector.items() if y > 0])\n",
    "    ranked = vector_space.get_ranking(vector_space.get_query_vector(query_terms))\n",
    "    ranked.reverse()\n",
    "    print(str([(x,y) for x,y in ranked if y > 0.0000]))\n",
    "    ranked = set([str(x) for x,y in ranked if y > 0.0000])\n",
    "    \n",
    "    print('Normal results')\n",
    "    print(f'results length :  {len(ranked)}')\n",
    "    print('Result Differnece : ')\n",
    "    print(t[1].difference(ranked))\n",
    "    print('Symmetric Difference : ')\n",
    "    print(t[1].symmetric_difference(ranked))\n",
    "    print()\n",
    "    normal_cnt = [len(t[1]), len(ranked.difference(t[1])), len(t[1].difference(ranked))]\n",
    "    normal_p = normal_cnt[0] / (normal_cnt[0] + normal_cnt[1])\n",
    "    normal_r = normal_cnt[0] / (normal_cnt[0] + normal_cnt[2])\n",
    "    print(f'precision : {normal_p}')\n",
    "    print(f'recall : {normal_r}')\n",
    "    cumulative_precision_normal += normal_p\n",
    "    cumulative_recall_normal += normal_r\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "f1_normal = (2 * (cumulative_precision_normal / cnt) * (cumulative_recall_normal / cnt) ) / (cumulative_recall_normal+cumulative_precision_normal)\n",
    "\n",
    "\n",
    "print(f'normal f1 : {f1_normal}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter Your Query  : muslims\n",
      "\t['muslim']\n",
      "['muslim']\n",
      "tf_idf muslim: 0.9542425094393249\n",
      "[(3, 0.06847745290157702), (4, 0.040067128561783606), (9, 0.03266160626131962), (2, 0.022462818788261157), (20, 0.01559631871326888), (6, 0.008682179111972119), (7, 0.008577975818554469)]\n"
     ]
    }
   ],
   "source": [
    "query = input('Enter Your Query  : ')\n",
    "lem = WordNetLemmatizer()\n",
    "query_terms = [lem.lemmatize(x.lower()) for x in query.split(' ')]\n",
    "print('\\t' + str(query_terms))\n",
    "# cs = vector_space.get_cosine_sim(vector_space.docs[1], query_vector)\n",
    "# print([(x,y) for x,y in query_vector.items() if y > 0])\n",
    "ranked = vector_space.get_ranking(vector_space.get_query_vector(query_terms))\n",
    "ranked.reverse()\n",
    "print(str([(x,y) for x,y in ranked if y > 0.0005]))\n",
    "ranked = set([str(x) for x,y in ranked if y > 0.0005])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References :\n",
    "\n",
    "http://www.pyregex.com/\n",
    "http://cs231n.github.io/python-numpy-tutorial/\n",
    "\n",
    "https://www.online-utility.org/text/analyzer.jsp\n",
    "\n",
    "https://stackoverflow.com/questions/2118261/parse-boolean-arithmetic-including-parentheses-with-regex\n",
    "\n",
    "https://regex101.com/r/M8z3U4/1\n",
    "\n",
    "https://iq.opengenus.org/porter-stemmer/\n",
    "\n",
    "https://unnikked.ga/how-to-build-a-boolean-expression-evaluator-518e9e068a65\n",
    "\n",
    "https://ruslanspivak.com/lsbasi-part7/\n",
    "\n",
    "https://github.com/gintas/django-picklefield"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "613.4px",
    "left": "645.6px",
    "right": "20px",
    "top": "84px",
    "width": "604.8px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
